{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from skimage import io\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import sys\n",
    "\n",
    "tf.test.gpu_device_name()\n",
    "print (tf.__version__)\n",
    "tf.config.experimental.list_physical_devices(device_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic way to create custom callback\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets = ['Orig', 'Image_th', 'Filter_RGB', 'Processed']\n",
    "datasets = [  'all_orig']\n",
    "#Models = [  'Resnet', 'CheXNet','Dense','Inception', 'VGG']\n",
    "Models = [  'Dense','Inception', 'VGG', 'CheXNet']\n",
    "losses = {'Dense-Orig':[], 'Dense-Image_th':[], 'Dense-Filter_RGB':[], 'Dense-Processed':[], \n",
    "         'VGG-Orig':[], 'VGG-Image_th':[], 'VGG-Filter_RGB':[], 'VGG-Processed':[], \n",
    "         'Inception-Orig':[], 'Inception-Image_th':[], 'Inception-Filter_RGB':[], 'Inception-Processed':[], \n",
    "         'Resnet-Orig':[], 'Resnet-Image_th':[], 'Resnet-Filter_RGB':[], 'Resnet-Processed':[],\n",
    "         'CheXNet-Orig':[], 'CheXNet-Image_th':[], 'CheXNet-Filter_RGB':[], 'CheXNet-Processed':[], \n",
    "         'Dense-all_orig':[], 'VGG-all_orig':[], 'Inception-all_orig':[], 'Resnet-all_orig':[], 'CheXNet-all_orig':[]}\n",
    "\n",
    "# specify image size and channels\n",
    "img_channels = 3\n",
    "img_rows = 299\n",
    "img_cols = 299\n",
    "\n",
    "# number of classes\n",
    "nb_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================\n",
      "Executing Dense for dataset- all_orig\n",
      "=========================================================================\n",
      "./data/all_orig/train\n",
      "./data/all_orig/test\n",
      "Dense\n",
      "./data/all_orig/train\n",
      "Found 10912 images belonging to 3 classes.\n",
      "Found 2727 images belonging to 3 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74842112/74836368 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "341/341 [==============================] - 285s 785ms/step - loss: 1.3424 - val_loss: 0.4415\n",
      "Epoch 2/2\n",
      "341/341 [==============================] - 264s 773ms/step - loss: 0.5577 - val_loss: 1.0684\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 285s 794ms/step - loss: 1.7855 - accuracy: 0.8638 - val_loss: 0.3054 - val_accuracy: 0.9286\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.92857, saving model to ./models_img/Dense-all_orig.hdf5\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 267s 783ms/step - loss: 0.1814 - accuracy: 0.9451 - val_loss: 0.3642 - val_accuracy: 0.9196\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.92857\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 261s 764ms/step - loss: 0.2440 - accuracy: 0.9481 - val_loss: 0.0971 - val_accuracy: 0.9702\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.92857 to 0.97024, saving model to ./models_img/Dense-all_orig.hdf5\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 267s 781ms/step - loss: 0.1312 - accuracy: 0.9650 - val_loss: 0.1443 - val_accuracy: 0.9643\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.97024\n",
      "Epoch 5/20\n",
      "341/341 [==============================] - 266s 779ms/step - loss: 0.0895 - accuracy: 0.9713 - val_loss: 0.1591 - val_accuracy: 0.9509\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.97024\n",
      "Epoch 6/20\n",
      "341/341 [==============================] - 264s 773ms/step - loss: 0.0585 - accuracy: 0.9794 - val_loss: 0.1264 - val_accuracy: 0.9628\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.97024\n",
      "Epoch 7/20\n",
      "341/341 [==============================] - 256s 749ms/step - loss: 0.0545 - accuracy: 0.9804 - val_loss: 0.1664 - val_accuracy: 0.9643\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.97024\n",
      "Epoch 8/20\n",
      "341/341 [==============================] - 257s 752ms/step - loss: 0.0527 - accuracy: 0.9820 - val_loss: 0.1682 - val_accuracy: 0.9628\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.97024\n",
      "Epoch 9/20\n",
      "341/341 [==============================] - 253s 742ms/step - loss: 0.0393 - accuracy: 0.9839 - val_loss: 0.1030 - val_accuracy: 0.9732\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.97024 to 0.97321, saving model to ./models_img/Dense-all_orig.hdf5\n",
      "Epoch 10/20\n",
      "341/341 [==============================] - 251s 735ms/step - loss: 0.0362 - accuracy: 0.9869 - val_loss: 0.1180 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.97321\n",
      "Epoch 11/20\n",
      "341/341 [==============================] - 261s 763ms/step - loss: 0.0373 - accuracy: 0.9877 - val_loss: 0.1319 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.97321\n",
      "Epoch 12/20\n",
      "341/341 [==============================] - 272s 797ms/step - loss: 0.0334 - accuracy: 0.9880 - val_loss: 0.1679 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.97321\n",
      "Epoch 13/20\n",
      "341/341 [==============================] - 270s 791ms/step - loss: 0.0357 - accuracy: 0.9870 - val_loss: 0.1609 - val_accuracy: 0.9613\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.97321\n",
      "Epoch 14/20\n",
      "341/341 [==============================] - 273s 800ms/step - loss: 0.0340 - accuracy: 0.9861 - val_loss: 0.0615 - val_accuracy: 0.9762\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.97321 to 0.97619, saving model to ./models_img/Dense-all_orig.hdf5\n",
      "Epoch 15/20\n",
      "341/341 [==============================] - 272s 795ms/step - loss: 0.0279 - accuracy: 0.9874 - val_loss: 0.0956 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.97619\n",
      "Epoch 16/20\n",
      "341/341 [==============================] - 270s 790ms/step - loss: 0.0266 - accuracy: 0.9888 - val_loss: 0.1459 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.97619\n",
      "Epoch 17/20\n",
      "341/341 [==============================] - 268s 785ms/step - loss: 0.0279 - accuracy: 0.9888 - val_loss: 0.1518 - val_accuracy: 0.9732\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.97619\n",
      "Epoch 18/20\n",
      "341/341 [==============================] - 262s 768ms/step - loss: 0.0173 - accuracy: 0.9955 - val_loss: 0.1216 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.97619\n",
      "Epoch 19/20\n",
      "341/341 [==============================] - 264s 774ms/step - loss: 0.0253 - accuracy: 0.9920 - val_loss: 0.1731 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.97619\n",
      "Epoch 20/20\n",
      "341/341 [==============================] - 261s 763ms/step - loss: 0.0130 - accuracy: 0.9944 - val_loss: 0.1601 - val_accuracy: 0.9643\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.97619\n",
      "[0.8469831347465515, 0.2889324128627777, 0.18570272624492645, 0.13539040088653564, 0.08881863951683044, 0.0599994994699955, 0.06051003560423851, 0.052467554807662964, 0.04624219983816147, 0.04101887717843056, 0.04058079048991203, 0.03573206439614296, 0.03196939080953598, 0.031185153871774673, 0.027916477993130684, 0.02680135890841484, 0.025520214810967445, 0.022559035569429398, 0.02300431579351425, 0.01757858693599701]\n",
      "Found 1514 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 2 2 2]\n",
      "[0 0 0 ... 2 2 2]\n",
      "=========================================================================\n",
      "Executing Inception for dataset- all_orig\n",
      "=========================================================================\n",
      "./data/all_orig/train\n",
      "./data/all_orig/test\n",
      "Inception\n",
      "./data/all_orig/train\n",
      "Found 10912 images belonging to 3 classes.\n",
      "Found 2727 images belonging to 3 classes.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "341/341 [==============================] - 253s 722ms/step - loss: 2.9415 - val_loss: 1.0959\n",
      "Epoch 2/2\n",
      "341/341 [==============================] - 248s 727ms/step - loss: 1.3378 - val_loss: 1.4196\n",
      "Epoch 1/20\n",
      "341/341 [==============================] - 260s 745ms/step - loss: 0.8063 - accuracy: 0.8855 - val_loss: 0.5627 - val_accuracy: 0.8988\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89881, saving model to ./models_img/Inception-all_orig.hdf5\n",
      "Epoch 2/20\n",
      "341/341 [==============================] - 254s 745ms/step - loss: 0.1827 - accuracy: 0.9484 - val_loss: 0.1239 - val_accuracy: 0.9658\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.89881 to 0.96577, saving model to ./models_img/Inception-all_orig.hdf5\n",
      "Epoch 3/20\n",
      "341/341 [==============================] - 249s 729ms/step - loss: 0.1251 - accuracy: 0.9608 - val_loss: 0.1254 - val_accuracy: 0.9613\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.96577\n",
      "Epoch 4/20\n",
      "341/341 [==============================] - 249s 731ms/step - loss: 0.0991 - accuracy: 0.9683 - val_loss: 0.1580 - val_accuracy: 0.9613\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.96577\n",
      "Epoch 5/20\n",
      "108/341 [========>.....................] - ETA: 2:41 - loss: 0.0798 - accuracy: 0.9690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341/341 [==============================] - 249s 731ms/step - loss: 0.0718 - accuracy: 0.9750 - val_loss: 0.0817 - val_accuracy: 0.9717\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.96577 to 0.97173, saving model to ./models_img/Inception-all_orig.hdf5\n",
      "Epoch 7/20\n",
      " 48/341 [===>..........................] - ETA: 3:19 - loss: 0.0778 - accuracy: 0.9738"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications import  DenseNet201 ,InceptionV3, DenseNet121, ResNet101\n",
    "from tensorflow.keras.layers import Input, Add, AveragePooling2D,  GlobalAveragePooling2D ,GlobalMaxPool2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def get_model(model, input_shape = (img_rows,img_cols,img_channels)):\n",
    "    #Get base model: ResNet50 or VGG16\n",
    "    finetune = True\n",
    "    chex = False\n",
    "    if model == 'Dense':\n",
    "        base_model =  DenseNet201(weights='imagenet', include_top=False ,input_shape=input_shape) \n",
    "        split_at = 481\n",
    "    elif model == 'VGG':\n",
    "        base_model =  VGG19(weights='imagenet', include_top=False ,input_shape=input_shape)\n",
    "        split_at = -8\n",
    "        finetune = False\n",
    "    elif model == 'Inception':\n",
    "        base_model =  InceptionV3(weights='imagenet', include_top=False ,input_shape=input_shape) \n",
    "        split_at = 249\n",
    "    elif model == 'Resnet':\n",
    "        base_model =  ResNet101(weights='imagenet', include_top=False ,input_shape=input_shape)\n",
    "        split_at = 313\n",
    "    elif model == 'CheXNet':\n",
    "        base_model = DenseNet121( include_top=False, weights=None, input_shape=input_shape )\n",
    "        split_at = 313\n",
    "        chex = True\n",
    "    else :\n",
    "        return 'No model'\n",
    "\n",
    "\n",
    "    # Get the output from the base model \n",
    "    base_model_output = base_model.output\n",
    "\n",
    "    x = base_model.output\n",
    "    x = AveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    predictions = Dense(3, activation= 'softmax')(x)\n",
    "\n",
    "#     dependencies = {\n",
    "#         'ValidAccuracy': ValidAccuracy\n",
    "#     }\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "#Finetuning\n",
    "    # first: train only the top layers (which were randomly initialized)\n",
    "    # i.e. freeze all convolutional InceptionV3 layers\n",
    "    if finetune :\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "        # train the model on the new data for a few epochs\n",
    "        model.fit_generator(training_set,  epochs=2, validation_data=validation_set, validation_steps=21)\n",
    "        if chex :\n",
    "            model.load_weights('./data/brucechou1983_CheXNet_Keras_0.3.0_weights.h5',by_name=True, skip_mismatch=True)\n",
    "\n",
    "        # let's visualize layer names and layer indices to see how many layers\n",
    "        # we should freeze:\n",
    "#         for i, layer in enumerate(base_model.layers):\n",
    "#            print(i, layer.name)\n",
    "#fine tune end============================================\n",
    "    \n",
    "    for layer in model.layers[:split_at]:\n",
    "       layer.trainable = False\n",
    "    for layer in model.layers[split_at:]:\n",
    "       layer.trainable = True\n",
    "\n",
    "    #print (model.summary())\n",
    "    return model\n",
    "#==========end of model creation==================================\n",
    "#================Looped iteration================================\n",
    "for modeltype in Models:\n",
    "    for dataset in datasets:\n",
    "        print ('=========================================================================')\n",
    "        print ('Executing ' + str(modeltype) + ' for dataset- ' + str(dataset))\n",
    "        print ('=========================================================================')\n",
    "        \n",
    "        # path to your dataset\n",
    "        Train_DATASET_PATH = './data/'+ str(dataset) + '/train'\n",
    "        TEST_DATASET = './data/'+ str(dataset) + '/test'\n",
    "        data_cls = ['COVID', 'Normal', 'Viral']\n",
    "\n",
    "        # globbing example\n",
    "        # help(glob)\n",
    "        #covid_path = os.path.join(Train_DATASET_PATH, data_cls[0], '*')\n",
    "        print(Train_DATASET_PATH)\n",
    "        print(TEST_DATASET)\n",
    "        print(modeltype)\n",
    "\n",
    "\n",
    "        #Create the datagenerator\n",
    "        datagen = ImageDataGenerator(rescale=1. / 255, validation_split=0.2,\n",
    "                                     rotation_range=20,\n",
    "                                     width_shift_range=0.1,\n",
    "                                     height_shift_range=0.1,\n",
    "                                     shear_range=0.1,\n",
    "                                     zoom_range=0.1,\n",
    "                                     horizontal_flip=True)\n",
    "        print (Train_DATASET_PATH)\n",
    "        training_set = datagen.flow_from_directory(\n",
    "                Train_DATASET_PATH,\n",
    "                target_size=(299, 299),\n",
    "                batch_size=32,shuffle=True,\n",
    "                class_mode='categorical',\n",
    "                subset='training')\n",
    "\n",
    "        validation_set = datagen.flow_from_directory(\n",
    "            Train_DATASET_PATH,\n",
    "            target_size=(299, 299),\n",
    "            batch_size=32,shuffle=True,\n",
    "            class_mode='categorical',\n",
    "            subset='validation')\n",
    "        #================================================================================================\n",
    "        #Get Model\n",
    "\n",
    "        model = get_model(modeltype)\n",
    "        opt = tf.keras.optimizers.SGD()\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=opt,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # generators\n",
    "        #training_generator = DataGenerator('train')#, ablation=50)\n",
    "        #validation_generator = DataGenerator('val')#, ablation=50)\n",
    "        #print (training_generator)\n",
    "\n",
    "        history = LossHistory()\n",
    "        plot_data = {}\n",
    "        filepath = './models_img/'+str(modeltype)+'-'+str(dataset)+'.hdf5'\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "        # fit\n",
    "        hist = model.fit_generator(training_set,  epochs=20, validation_data=validation_set, validation_steps=21, callbacks=[history, checkpoint])\n",
    "        # for key in hist.history:\n",
    "        #    print(key)\n",
    "        #================================================================================================\n",
    "\n",
    "        #print (len(history.losses))\n",
    "        print (history.losses)\n",
    "        losses[str(modeltype)+'-'+str(dataset)].append (history.losses)\n",
    "\n",
    "        #======================================================================================================\n",
    "        #load  model for test\n",
    "        #using the inbuilt imagegenerator\n",
    "        import h5py\n",
    "        from tensorflow.keras.models import load_model\n",
    "        from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "        #from tensorflow.math import confusion_matrix\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "        test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        test_set = test_datagen.flow_from_directory(\n",
    "                TEST_DATASET,\n",
    "                target_size=(299, 299),\n",
    "                batch_size=32,shuffle=False,\n",
    "                class_mode='categorical')\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        test_set.reset() \n",
    "\n",
    "        model = load_model('./models_img/'+str(modeltype)+'-'+str(dataset)+'.hdf5')\n",
    "\n",
    "        Y_pred = model.predict_generator(test_set,1514 // 32+1)\n",
    "        y_pred = np.argmax(Y_pred,axis=-1)\n",
    "        s=confusion_matrix(test_set.classes,y_pred)\n",
    "\n",
    "        #=====================================================================================================\n",
    "        #Display results for test\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        target_names = data_cls\n",
    "        print (test_set.classes)\n",
    "        print (y_pred)\n",
    "        c = classification_report(test_set.classes.tolist(),y_pred.tolist(),target_names=target_names)\n",
    "\n",
    "        with open('results.txt', 'a') as f:\n",
    "            print (str(modeltype) + '-' + str(dataset), file=f)\n",
    "            print ('==============================================================================', file=f)\n",
    "            print (s, file=f)\n",
    "            print (c, file=f)\n",
    "            #print ('Accuracy------ ' )\n",
    "            print ('Test Accuracy------ '+ str(accuracy_score(test_set.classes.tolist(),y_pred.tolist())), file=f)\n",
    "            print ('loss ------------', file=f)\n",
    "            print (history.losses, file=f)\n",
    "            print ('=============================End of run========================================', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
