{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLW1oqiuaPpX"
   },
   "source": [
    "## Telecom Churn Case study\n",
    "- The objetive is to identify the customer which are expected to Churn in the high value customers\n",
    "- Second Objective is to idenitfy the factores impacting the Churn of these customers\n",
    "\n",
    "Steps to be followed\n",
    "- Derieve new features based on the data available to identify the Churn\n",
    "- Based on the dataset provided identify the high value customers and remove all the other records - 29.9k records. Customer who have recharged more than 70th percentile of the averge recharg in the first 2 months.\n",
    "- Remove the attributes of the Churn month from the dataset\n",
    "- Preprocess data (convert columns to appropriate formats, handle missing values, etc.)\n",
    "- Conduct appropriate exploratory analysis to extract useful insights (whether directly useful for business or for eventual modelling/feature engineering).\n",
    "- Derive new features.\n",
    "- Reduce the number of variables using PCA.\n",
    "- Train a variety of models, tune model hyperparameters, etc. (handle class imbalance using appropriate techniques).\n",
    "- Evaluate the models using appropriate evaluation metrics. Note that is is more important to identify churners than the non-churners accurately - choose an appropriate evaluation metric which reflects this business goal.\n",
    "- Finally, choose a model based on some evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLaW1-1nuP0R"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zf6okPoOaPpb"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWbuCqiMb24D"
   },
   "outputs": [],
   "source": [
    "#pip install -U pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IF1G9ueuaPph"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score ,confusion_matrix ,f1_score ,recall_score,precision_score ,classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "#import pandas_profiling \n",
    "\n",
    "pd.set_option('display.max_columns', 999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "_itmMXIfaPpo",
    "outputId": "d92c6ae9-5c22-4cff-90bb-28a3c0c20e44"
   },
   "outputs": [],
   "source": [
    "df_org = pd.read_csv(\"telecom_churn_data.csv\",encoding='cp1252')\n",
    "#df_org = pd.read_csv('/content/sample_data/telecom_churn_data.csv' ,encoding='cp1252')\n",
    "df_org.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4C-9QDI2aPpz"
   },
   "source": [
    "## 1 Data preparation\n",
    "- Missing value treatment\n",
    "- Managing the Categorical date variables\n",
    "- Create new features such a total recharge etc.\n",
    "- Identify the High Value Customers\n",
    "- Outliers Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXER_aK4aPp1"
   },
   "outputs": [],
   "source": [
    "#Drop the columns - last_date_of_month_7,last_date_of_month_6,last_date_of_month_8,last_date_of_month_9 , circle_id\n",
    "#as these have constant values which is not of much use in modeling\n",
    "df_org.drop(['last_date_of_month_6','last_date_of_month_7','last_date_of_month_8','last_date_of_month_9','circle_id']\n",
    "            ,inplace=True\n",
    "           ,axis=1)\n",
    "\n",
    "\n",
    "df_org.date_of_last_rech_6 = df_org.date_of_last_rech_6.apply(lambda x:1 if not pd.isnull(x) else 0)\n",
    "df_org.date_of_last_rech_7 = df_org.date_of_last_rech_7.apply(lambda x:1 if not pd.isnull(x) else 0)\n",
    "df_org.date_of_last_rech_8 = df_org.date_of_last_rech_8.apply(lambda x:1 if not pd.isnull(x) else 0)\n",
    "df_org.date_of_last_rech_9 = df_org.date_of_last_rech_9.apply(lambda x:1 if not pd.isnull(x) else 0)\n",
    "\n",
    "df_org.date_of_last_rech_data_6 = df_org.date_of_last_rech_data_6.apply(lambda x:1 if not pd.isnull(x) else 0)\n",
    "df_org.date_of_last_rech_data_7 = df_org.date_of_last_rech_data_7.apply(lambda x:1 if not pd.isnull(x) else 0)\n",
    "df_org.date_of_last_rech_data_8 = df_org.date_of_last_rech_data_8.apply(lambda x:1 if not pd.isnull(x) else 0)\n",
    "df_org.date_of_last_rech_data_9 = df_org.date_of_last_rech_data_9.apply(lambda x:1 if not pd.isnull(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVIb7HjPaPp6"
   },
   "source": [
    "Convert the Date field to an integer since this will only store a 0 or 1 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLFMGpF1aPqB"
   },
   "outputs": [],
   "source": [
    "#Convert the Date columns to Int\n",
    "col_dict = {'date_of_last_rech_6': int,\n",
    "            'date_of_last_rech_7': int, \n",
    "            'date_of_last_rech_8': int,\n",
    "            'date_of_last_rech_9': int,\n",
    "            'date_of_last_rech_data_6': int, \n",
    "            'date_of_last_rech_data_7': int,\n",
    "            'date_of_last_rech_data_8': int,\n",
    "            'date_of_last_rech_data_9': int\n",
    "           } \n",
    "\n",
    "df_org = df_org.astype(col_dict) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuJeArLiaPqG"
   },
   "source": [
    "- Looking at the Data , it is clear that above 74% of custmers do not have any information on the Data usage (around 40 columns). It is likely that these customer do not use the data service.For such cols , there are few relatd to data which are needed to identify the high value customer. These columns can be retained and the other ones dropped. \n",
    "- There are columns which have more than 7 percent of data issing which are majorly features for the month 9. these could be due to customer who have churned. We will impute the data as 0 for the null values. It will not make much of a difference since this columsn will be dropped later\n",
    "- There are few columns which around 5.378 prcent missing data and are call features belonging to month 8.We impute these columns as 0. These could be potential churn customers in month 8 and 9.\n",
    "- Features such as 'Last Date of the month' can be dropped as they have constant data which will not be of much use\n",
    "- Voice calls related few columns have missing data between 1-4 percent. These are also imputed as 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ug2w4esaPqH"
   },
   "outputs": [],
   "source": [
    "#Impute 0 in  columns which are needed for calculating the high value customers.\n",
    "#set the columns name to only few features needed for the calculation of high value cust\n",
    "column_name = ['av_rech_amt_data_6','av_rech_amt_data_7','av_rech_amt_data_8','total_rech_data_6','total_rech_data_7','total_rech_data_8','total_rech_data_9']\n",
    "for col in column_name:\n",
    "  df_org[col].fillna(value = 0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "colab_type": "code",
    "id": "Iy35jydvaPqM",
    "outputId": "c227c154-b847-43c3-c687-524a575dc0ee"
   },
   "outputs": [],
   "source": [
    "#Remove the columsn with 70% missing data\n",
    "col_missing = df_org.isnull().sum() * 100 / len(df_org)\n",
    "df_missing = pd.DataFrame({'column_name': df_org.columns,\n",
    "                                 'percent_missing': col_missing})\n",
    "\n",
    "df_missing.sort_values('percent_missing', ascending=False)\n",
    "print(len(df_missing[df_missing.percent_missing > 70]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYzwcR2AaPqm"
   },
   "source": [
    "New features such as the following created from the dataset\n",
    "- Create a new feature which holds the total recharge for the customer for the months 6 and 7. This will provide to calculate the High Value customers.\n",
    "- It is assumed here the 'total_rech_amt_7'and 'total_rech_amt_6' do not  include the recharge due to data. There are some rows where the 'total_rech_amt_6' is less that the (avg data * Total number of recharge ).\n",
    "- Hence the Totalmaount has been calculated as df_org.total_rech_amt_7 + df_org.total_rech_amt_6 +                         (df_org['av_rech_amt_data_6'] * df_org['total_rech_data_6']) + (df_org['av_rech_amt_data_7'] * df_org['total_rech_data_7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NTb7b3vmaPqo"
   },
   "outputs": [],
   "source": [
    "#Create some additional features\n",
    "df_org['Total_Rech_6_7'] = df_org.total_rech_amt_7 + df_org.total_rech_amt_6 + \\\n",
    "                        (df_org['av_rech_amt_data_6'] * df_org['total_rech_data_6']) + \\\n",
    "                        (df_org['av_rech_amt_data_7'] * df_org['total_rech_data_7'])\n",
    "df_org['TotalRech_data_6'] = (df_org['av_rech_amt_data_6'] * df_org['total_rech_data_6'])\n",
    "df_org['TotalRech_data_7'] = (df_org['av_rech_amt_data_7'] * df_org['total_rech_data_7'])\n",
    "df_org['TotalRech_data_8'] = (df_org['av_rech_amt_data_8'] * df_org['total_rech_data_8'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HZNseCyPaPqs"
   },
   "outputs": [],
   "source": [
    "#Impute the columns withmore than 70% missing data and set the values to 0. \n",
    "#This is since most of these columns are empty since the service is not taken by the customer.\n",
    "for col in np.array(df_missing[df_missing.percent_missing > 70]['column_name']):\n",
    "    df_org.drop(col,inplace=True,axis=1)\n",
    "\n",
    "#Impute the remaing columns with 0\n",
    "df_org.fillna(value = 0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_chBEd-aPq5"
   },
   "source": [
    "To identify the high value customers, the total recharge column for each customer has been calculated for the months 6 and 7. This has been saved in the new feature created 'Total_Rech_6_7'. Any customer who has re-charged more (sum of recharges in 6 and 7) than 70th perentile of this feature is classified as the 'High Value customer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAAhFgG-aPq6"
   },
   "outputs": [],
   "source": [
    "df_highvalue = df_org[(df_org.Total_Rech_6_7) > (df_org.Total_Rech_6_7.quantile(0.7) )]\n",
    "#Drop the new 'Total recharge' column as we already have the high value customers identified\n",
    "df_highvalue.drop('Total_Rech_6_7', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qzhDVzURaPq-",
    "outputId": "c79df064-820d-4d05-b93e-ae4832e2218f"
   },
   "outputs": [],
   "source": [
    "df_highvalue.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DsB98QzaPrE"
   },
   "source": [
    "#### Total number of High Values records comes to 29953 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_4Uo08aaPrG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQFVmIU8aPrJ"
   },
   "source": [
    "Identify the Churn customers in the 9th month by checking the values for the four columsn such as - 'total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9'.\n",
    "\n",
    "If the Values is zero for these columns then the customer of a 'Churn' class. \n",
    "\n",
    "Create a new feature - Churn and update it as 1 for all Churn customers and 0 for non churn customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoZBSm0VaPrL"
   },
   "outputs": [],
   "source": [
    "#identify the case whichhave churned already\n",
    "cols_9 = ['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']\n",
    "df_highvalue['churn'] = df_highvalue[cols_9].sum(axis=1)\n",
    "#df_highvalue.loc[df_highvalue.churn.nonzero()].churn = 1\n",
    "df_highvalue.churn = df_highvalue.churn.apply(lambda x:1 if x == 0 else 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yWK0JJ5zaPrP",
    "outputId": "2c76dc44-708e-42af-92d1-26cb79a27d9c"
   },
   "outputs": [],
   "source": [
    "#ratio of  churn Vs non Churn in the dataset\n",
    "df_highvalue[df_highvalue.churn == 1].mobile_number.count() / df_highvalue[df_highvalue.churn == 0].mobile_number.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uiYibIViaPrT"
   },
   "source": [
    "As seen from the Churn / Non Churn ratio, the dataset is highly imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qX4_mqZiaPrV",
    "outputId": "c13a8643-4a11-4e9d-bd01-6b526a63a83f"
   },
   "outputs": [],
   "source": [
    "#remove the attributes related to churn in month 9\n",
    "cols_9_drp = [c for c in df_highvalue.columns if c[-2:] == '_9']\n",
    "df_highvalue.drop(cols_9_drp,axis=1, inplace=True)\n",
    "df_highvalue.drop('sep_vbc_3g',axis=1, inplace=True)\n",
    "df_highvalue.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBZlhTIPPVIl"
   },
   "outputs": [],
   "source": [
    "df_highvalue.drop('mobile_number',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4n2FameFAYtD"
   },
   "outputs": [],
   "source": [
    "#remove the columns which have only 1 unique values. These columns will not matter since they are constant.\n",
    "for col in df_highvalue.columns:\n",
    "  if df_highvalue[col].nunique() == 1:\n",
    "    df_highvalue.drop(col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Z0WTUL4KxcYU",
    "outputId": "c14904d0-3960-40d8-8c16-0bde5057344d"
   },
   "outputs": [],
   "source": [
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_highvalue)\n",
    "profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "colab_type": "code",
    "id": "Ccbz2gM6aPrZ",
    "outputId": "7184d8e3-fd06-4d20-9c14-da5fd2a2bac6"
   },
   "outputs": [],
   "source": [
    "df_highvalue.describe(percentiles=[0.01,0.05,0.10,0.20,0.30,0.50,0.75, 0.80, 0.85, 0.90,0.95,0.99])\n",
    "#ARPU can be negative and hecne there is no treatment done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZopeWvKwaPrc"
   },
   "source": [
    "## 2 Exploratory data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "Z4Z5Dsz-aPre",
    "outputId": "139f4b4b-bdbe-4658-e698-0d706ff29f34"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Check the data for ARPU across 3 months\n",
    "s= sns.boxplot(data=df_highvalue[['arpu_6','arpu_7','arpu_8']])\n",
    "s.set_yscale(\"log\")\n",
    "#Almost same across the set for months 6,7,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "colab_type": "code",
    "id": "lP2NoUOZaPrj",
    "outputId": "4a13a3f3-6abc-4cef-a2fb-2af5a7b09920"
   },
   "outputs": [],
   "source": [
    "#Customer spend for data VS Voice across months 6,7,8\n",
    "plt.figure(figsize=(10,10))\n",
    "s = sns.boxplot(data=df_highvalue[['total_rech_amt_6','total_rech_amt_7','total_rech_amt_8','TotalRech_data_6' \\\n",
    "                              ,'TotalRech_data_7','TotalRech_data_8']])\n",
    "s.set_yscale(\"log\")\n",
    "#Data revenue is slightly lower than the voice revenue\n",
    "#However, in the month 8 voice revenue has not reduced but the data revenue has reduced. \n",
    "#This could mean that the customer's are not happy with the Data service and hence this is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsnCI1raSRks"
   },
   "source": [
    "Finding 1 - The above plot shows that there is a reduction in the Data usage in th month 8. This is due to the customers who have churned in month 9, and hecne reduced the usage in the month 8. Most of the reduction is in data. Hence it can be concluded that Data is a key reason why customers have churned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "1Ip0CKyHaPrs",
    "outputId": "ca9d088b-13e5-4b83-fd69-ae8259b12cef"
   },
   "outputs": [],
   "source": [
    "#Check for the Voice Recharge across churn and no  churn users in 6 , 7 , 8\n",
    "a, num = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "box1 = sns.boxplot(x='churn' ,y='total_rech_amt_6' ,data=df_highvalue,ax=num[0])\n",
    "box1.set_yscale(\"log\")\n",
    "box2 = sns.boxplot(x='churn' ,y='total_rech_amt_7' ,data=df_highvalue,ax=num[1])\n",
    "box2.set_yscale(\"log\")\n",
    "box3 = sns.boxplot(x='churn' ,y='total_rech_amt_8' ,data=df_highvalue,ax=num[2])\n",
    "box3.set_yscale(\"log\")\n",
    "#Plot suggests that the Churn customers data in month 8 has reduced while the revenue \n",
    "#for non-churn is maintaned at the same level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_YAT4UZS9cS"
   },
   "source": [
    "Finding 2 - Potential Churn customers have reduced the usage of the network in month 8 and this could provide an indication of churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "Oei16t3_aPrx",
    "outputId": "a678a871-6625-4d15-d99d-7d4495d0c059"
   },
   "outputs": [],
   "source": [
    "#Check for the Recharge across churn and no  churn users in 6 , 7 , 8\n",
    "a, num = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "box1 = sns.boxplot(x='churn' ,y='TotalRech_data_6' ,data=df_highvalue,ax=num[0])\n",
    "box1.set_yscale(\"log\")\n",
    "box2 = sns.boxplot(x='churn' ,y='TotalRech_data_7' ,data=df_highvalue,ax=num[1])\n",
    "box2.set_yscale(\"log\")\n",
    "box3 = sns.boxplot(x='churn' ,y='TotalRech_data_8' ,data=df_highvalue,ax=num[2])\n",
    "box3.set_yscale(\"log\")\n",
    "#Significant fall in the recharge revenue for the customers in month 8, who have churned in Month 9. \n",
    "#The revenue for the non-churn customers follows the same pattern as in month 6 and 7.\n",
    "#Seems like Data is a likely reason for drops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrt1CAK0aPr2"
   },
   "source": [
    "Finding 3 - Based on the above plot ,the Total recharge Data by the customer  is a significant influencer in identifying the churn of customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "Tp8N6V3YaPr3",
    "outputId": "5f94c6ce-4807-479e-d086-b13cd15b6877"
   },
   "outputs": [],
   "source": [
    "#Check the calls to call center in the months 6,7,8 - local\n",
    "\n",
    "a, num = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "sns.scatterplot(x='churn' ,y='loc_og_t2c_mou_6' ,data=df_highvalue ,ax=num[0])\n",
    "sns.scatterplot(x='churn' ,y='loc_og_t2c_mou_7' ,data=df_highvalue,ax=num[1])\n",
    "sns.scatterplot(x='churn' ,y='loc_og_t2c_mou_8' ,data=df_highvalue,ax=num[2])\n",
    "\n",
    "\n",
    "#Calls to the callcenter seems to be reduced in the month 8 for the churn customers.However the calls to callcenter probably to resolve their issues\n",
    "#int he month 7 is high for Churn customers. This indicates that the field could be used as a predictor for churn. \n",
    "#However , it needs to be seen in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "cVVGfmxgaPr7",
    "outputId": "d20c359c-c46b-4b29-8530-ed095dce28d4"
   },
   "outputs": [],
   "source": [
    "#Check the calls within network in the months 6,7,8 - local\n",
    "\n",
    "a, num = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "box1 = sns.boxplot(x='churn' ,y='onnet_mou_6' ,data=df_highvalue ,ax=num[0])\n",
    "box1.set_yscale(\"log\")\n",
    "\n",
    "box = sns.boxplot(x='churn' ,y='onnet_mou_7' ,data=df_highvalue,ax=num[1])\n",
    "box.set_yscale(\"log\")\n",
    "\n",
    "box = sns.boxplot(x='churn' ,y='onnet_mou_8' ,data=df_highvalue,ax=num[2])\n",
    "box.set_yscale(\"log\")\n",
    "\n",
    "#Reduced Onnet calls in month 8. this could be because of the lower calls for churn customer in month 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "pkvT-FXSaPsB",
    "outputId": "6d9f0b8c-f5df-45c3-bc64-f956d2a51a80"
   },
   "outputs": [],
   "source": [
    "#Check the calls within network in the months 6,7,8 - local\n",
    "\n",
    "a, num = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "box1 = sns.boxplot(x='churn' ,y='offnet_mou_6' ,data=df_highvalue ,ax=num[0])\n",
    "box1.set_yscale(\"log\")\n",
    "\n",
    "box = sns.boxplot(x='churn' ,y='offnet_mou_7' ,data=df_highvalue,ax=num[1])\n",
    "box.set_yscale(\"log\")\n",
    "\n",
    "box = sns.boxplot(x='churn' ,y='offnet_mou_8' ,data=df_highvalue,ax=num[2])\n",
    "box.set_yscale(\"log\")\n",
    "\n",
    "#Reduced Offnet calls in month 8. this could be because of the lower calls for churn customer in month 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "xXiPSTebaPsH",
    "outputId": "ea314595-da8d-42e0-d36a-7e823e8fd760"
   },
   "outputs": [],
   "source": [
    "#Check the age within network in the months 6,7,8\n",
    "\n",
    "#a, num = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "box1 = sns.boxplot(x='churn' ,y='aon' ,data=df_highvalue )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfyN_eX1aPsL"
   },
   "source": [
    "Finding 4\n",
    "- Age is generally lower for the customer leaving the network with lot of outliers. not much of difference\n",
    "- However the range suggests that most of the customer churning are less than 1000 days. Customer who have stayed with the network beyond 1000 tend to stay with the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "colab_type": "code",
    "id": "fqcHkZdraPsM",
    "outputId": "cae3db56-5f2e-4eaf-a7ae-0c12226d4eb6"
   },
   "outputs": [],
   "source": [
    "#Check the age within network in the months 8\n",
    "plt.figure(figsize=(20,25))\n",
    "a, num = plt.subplots(3, 3,figsize=(15,15))\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "sc_6 = sns.scatterplot(x='total_rech_amt_6' ,y='total_rech_data_6',hue='churn' ,data=df_highvalue ,ax=num[0,0])\n",
    "sc_7 = sns.scatterplot(x='total_rech_amt_7' ,y='total_rech_data_7',hue='churn' ,data=df_highvalue ,ax=num[0,1])\n",
    "sc_8 = sns.scatterplot(x='total_rech_amt_8' ,y='total_rech_data_8',hue='churn' ,data=df_highvalue ,ax=num[0,2])\n",
    "\n",
    "#a, num = plt.subplots(1, 3)\n",
    "#plt.subplots_adjust(wspace = 0.5)\n",
    "box_6 = sns.boxplot(x='churn' ,y='total_rech_data_6',data=df_highvalue ,ax=num[1,0])\n",
    "#box_6.set_yscale(\"log\")\n",
    "box_7 = sns.boxplot(x='churn' ,y='total_rech_data_7',data=df_highvalue ,ax=num[1,1])\n",
    "#box_7.set_yscale(\"log\")\n",
    "box_8 = sns.boxplot(x='churn' ,y='total_rech_data_8',data=df_highvalue ,ax=num[1,2])\n",
    "#box_8.set_yscale(\"log\")\n",
    "\n",
    "box_6 = sns.boxplot(y='total_rech_amt_6' ,x='churn',data=df_highvalue ,ax=num[2,0])\n",
    "box_7 = sns.boxplot(y='total_rech_amt_7' ,x='churn',data=df_highvalue ,ax=num[2,1])\n",
    "box_8 = sns.boxplot(y='total_rech_amt_8' ,x='churn',data=df_highvalue ,ax=num[2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kG5Nq6KNaPsP"
   },
   "source": [
    "Finding 5 - The Churn customers spend reduced in the month 8 as seen from the subplots. there are very few orange points in the plot for month 8\n",
    "\n",
    "Finding 6 - Also,recharge for Data and Amount is lower as seen from the box plots as well.This had reduced progressivly from 6 to 8 month for the Churn customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WD5Oi90gaPsY"
   },
   "source": [
    "## 3 Splitting Data, correcting imbalance and scaling\n",
    "- Split the data into Train and Test\n",
    "- Mechanism to correct the imbalance in data using SMOTE\n",
    "- Scale the data \n",
    "- Create a interpretable model using the RFE and Logistic regression Ridge algorithm\n",
    "- Check for the key parameters using the PCA\n",
    "- Use multiple models such as logisticClassifier,RandomForest Classifier  and SVM with poly kernel to check for the best result \n",
    "- manage the confusion matrix to focus on the churn user rather than non churn. use recall score for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nWNxVM1WaPsa"
   },
   "outputs": [],
   "source": [
    "y = df_highvalue.churn\n",
    "X = df_highvalue.drop('churn',axis=1)\n",
    "#Perform the Train test split in the dataset before the data can be upsampled for the churn class\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OE24EYcwsK4"
   },
   "outputs": [],
   "source": [
    "#Handling Skew - Capping and flooring\n",
    "for col in X_train.columns:\n",
    "    p05 = X_train[col].quantile(.05)\n",
    "    p95 = X_train[col].quantile(.95)   \n",
    "    X_train[col] = np.where( X_train[col] < p05, p05, X_train[col])\n",
    "    X_train[col] = np.where(X_train[col] > p95 , p95, X_train[col])\n",
    "    X_test[col] = np.where( X_test[col] < p05, p05, X_test[col])\n",
    "    X_test[col] = np.where(X_test[col] > p95 , p95, X_test[col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnnaZQdWaPsf"
   },
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=40)\n",
    "col = X_train.columns\n",
    "X_train_1, y_train_1 = sm.fit_resample(X_train, y_train)\n",
    "df_X_train_upsampled = pd.DataFrame(X_train_1,columns=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IraeEJdIaPsj"
   },
   "outputs": [],
   "source": [
    "#Scale the Train and test data\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(df_X_train_upsampled)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "X_train_upsampled_sc = pd.DataFrame(X_train_scaled,columns=col)\n",
    "X_test_sc = pd.DataFrame(X_test_scaled,columns=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVcY76iGaPsm"
   },
   "source": [
    "## 4 Using RFE to identify the top 10 and Top 15 features.\n",
    "These features are then used with a Decision tree classifier to measure the performance on the test set with those features.\n",
    "Based on this - Top 10 features give a good recall score and it reduces for top 15 features, even though the accuracy increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "colab_type": "code",
    "id": "ttSj66PgaPsp",
    "outputId": "3b3133c5-21ea-4a42-def6-6907f55ccc92"
   },
   "outputs": [],
   "source": [
    "#Apply RFE using skitlearn\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "# Importing decision tree classifier from sklearn library\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for i in [10,15]:\n",
    "    estimator = LogisticRegression(C=0.1,max_iter=500,class_weight=\"balanced\", random_state=100)\n",
    "    rfe_feat = RFE(estimator,i)\n",
    "    fit = rfe_feat.fit(X_train_upsampled_sc,y_train_1)\n",
    "    fit.ranking_\n",
    "    df_rfe = pd.DataFrame(fit.ranking_,columns=['Ranking'])\n",
    "    df_col = pd.DataFrame(col,columns=['Feature'])\n",
    "    df_score_rfe = pd.concat([df_rfe,df_col],axis=1)\n",
    "    df_score_rfe.sort_values('Ranking',ascending=True,inplace=True)\n",
    "    print(df_score_rfe.head(20))\n",
    "\n",
    "    #use the RFE from the earlier steps and include those columns in the input for Training and Testing model\n",
    "    #Fitting the decision tree with default hyperparameters, apart from\n",
    "    #max_depth which is 5 to avoid overfitting\n",
    "\n",
    "    dt_default = DecisionTreeClassifier(max_depth=5,random_state=100)\n",
    "    dt_default.fit(X_train_upsampled_sc[df_score_rfe[df_score_rfe.Ranking ==1].Feature], y_train_1)\n",
    "    churn_pred = dt_default.predict(X_test_sc[df_score_rfe[df_score_rfe.Ranking ==1].Feature])\n",
    "    print (confusion_matrix(y_test,churn_pred))\n",
    "    print ('Accuracy score - {0}'.format(accuracy_score(y_test, churn_pred)))\n",
    "    print ('F1 score - {0}'.format(f1_score(y_test, churn_pred)))\n",
    "    print ('Recall score - {0}'.format(recall_score(y_test, churn_pred)))\n",
    "    print ('Precision score - {0}'.format(precision_score(y_test, churn_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "yp8qFqDim2mB",
    "outputId": "8f233e74-6df2-482e-f3cf-b331ae72b8d9"
   },
   "outputs": [],
   "source": [
    "#Use the Random classifier to identify the top featues influencing the model\n",
    "\n",
    "featsel_RFC = RandomForestClassifier(class_weight=\"balanced\",random_state=100)\n",
    "featsel_RFC.fit(X_train_upsampled_sc,y_train_1)\n",
    "features = []\n",
    "for feature in zip(X_train_upsampled_sc.columns, featsel_RFC.feature_importances_):\n",
    "    features.append (feature)\n",
    "df_features = pd.DataFrame(features, columns=['Feature', 'Ginni'])\n",
    "print (df_features.sort_values('Ginni', ascending=False).head(10))\n",
    "\n",
    "dt_default = DecisionTreeClassifier(max_depth=5,random_state=100)\n",
    "dt_default.fit(X_train_upsampled_sc[df_features['Feature']], y_train_1)\n",
    "churn_pred = dt_default.predict(X_test_sc[df_features['Feature']])\n",
    "print (confusion_matrix(y_test,churn_pred))\n",
    "print ('Accuracy score - {0}'.format(accuracy_score(y_test, churn_pred)))\n",
    "print ('F1 score - {0}'.format(f1_score(y_test, churn_pred)))\n",
    "print ('Recall score - {0}'.format(recall_score(y_test, churn_pred)))\n",
    "print ('Precision score - {0}'.format(precision_score(y_test, churn_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPDLvh4KaPst"
   },
   "source": [
    "RFE gives a very similar score for Top 10 Vs Top 15 features. We can go for the **top 10 features** as the key influencers.\n",
    "\n",
    "Top 10 features provides by RFE perform better as compared to features from RandomForestClassifier. This could be because the features are mostly linear. Recall Score is slighlty higher for the features selected by RFE method.\n",
    "\n",
    "Using the key features from both method, the list is as below -\n",
    "\n",
    "- loc_ic_t2m_mou_8\n",
    "- loc_ic_mou_7\n",
    "- loc_ic_t2t_mou_8\n",
    "- loc_ic_mou_8\n",
    "- total_rech_num_8\n",
    "- total_rech_num_7\n",
    "- spl_ic_mou_8\n",
    "- vol_2g_mb_8\n",
    "- aug_vbc_3g\n",
    "- roam_og_mou_8\n",
    "- roam_ic_mou_8\n",
    "- total_ic_mou_8\n",
    "- last_day_rch_amt_8\n",
    "- date_of_last_rech_data_8\n",
    "- loc_og_t2m_mou_8\n",
    "- loc_og_mou_8\n",
    "\n",
    "Next step is to identify the co-relation among these features and 'churn' and identify the key recommendation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "colab_type": "code",
    "id": "lzi-69YhaPsu",
    "outputId": "472e4c17-deb4-41f2-d2eb-fcf8b7151ce6"
   },
   "outputs": [],
   "source": [
    "RFE_cols = ['loc_ic_t2m_mou_8','loc_ic_mou_7','loc_ic_t2t_mou_8','loc_ic_mou_8','total_rech_num_8','total_rech_num_7' \\\n",
    "            ,'spl_ic_mou_8','vol_2g_mb_8','aug_vbc_3g','roam_og_mou_8','roam_ic_mou_8','total_ic_mou_8','last_day_rch_amt_8' \\\n",
    "            ,'date_of_last_rech_data_8','loc_og_t2m_mou_8','loc_og_mou_8','churn']\n",
    "corr = df_highvalue[RFE_cols].corr()\n",
    "# plot the heatmap\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns,\n",
    "        annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xwA_1DLLNq4"
   },
   "source": [
    "\n",
    "- Set 1 of highly co-related fieleds - ```'loc_ic_t2m_mou_8','loc_ic_mou_7','loc_ic_t2t_mou_8','loc_ic_mou_8'``` are highly co related as these are all Incoming calls. This is also corealted with ```'total_ic_mou_8'```\n",
    "- Set 2 - ```'total_rech_num_8','total_rech_num_7'``` are highly co-related\n",
    "- Set 3 - ```'roam_og_mou_8','roam_ic_mou_8'``` are highly co-related\n",
    "- Set 4 - ```'vol_2g_mb_8','aug_vbc_3g'``` are co-related with ```'date_of_last_rech_data_8'```\n",
    "- Set 5 - ```'last_day_rch_amt_8'``` is not co-related and impacts the Churn. No recharge in Month 8 indicates a Churn.\n",
    "- Set 6```'loc_og_t2m_mou_8','loc_og_mou_8'``` are highly co-related to each other and also to the incomming call features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "rLEbu4eGDgBx",
    "outputId": "a889e91c-9885-4a6e-bcc3-5858038b1408"
   },
   "outputs": [],
   "source": [
    "#Set 1 and 6 EDA - 'total_ic_mou_8' can be considered as one of the key predictors\n",
    "\n",
    "a, num = plt.subplots(2, 4,figsize=(15,5))\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "#a, num = plt.subplots(1, 3)\n",
    "#plt.subplots_adjust(wspace = 0.5)\n",
    "box_1 = sns.boxplot(x='churn' ,y='loc_ic_t2m_mou_8',data=df_highvalue ,ax=num[0,0])\n",
    "box_1.set_yscale(\"log\")\n",
    "box_2 = sns.boxplot(x='churn' ,y='loc_ic_mou_7',data=df_highvalue ,ax=num[0,1])\n",
    "box_2.set_yscale(\"log\")\n",
    "box_3 = sns.boxplot(x='churn' ,y='loc_ic_t2t_mou_8',data=df_highvalue ,ax=num[0,2])\n",
    "box_3.set_yscale(\"log\")\n",
    "box_4 = sns.boxplot(y='loc_ic_mou_8' ,x='churn',data=df_highvalue ,ax=num[0,3])\n",
    "box_4.set_yscale(\"log\")\n",
    "box_5 = sns.boxplot(y='total_ic_mou_8' ,x='churn',data=df_highvalue ,ax=num[1,0])\n",
    "box_5.set_yscale(\"log\")\n",
    "box_6 = sns.boxplot(y='loc_og_t2m_mou_8' ,x='churn',data=df_highvalue ,ax=num[1,1])\n",
    "box_6.set_yscale(\"log\")\n",
    "box_7 = sns.boxplot(y='loc_og_mou_8' ,x='churn',data=df_highvalue ,ax=num[1,2])\n",
    "box_7.set_yscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "wrCQDf5_HxyK",
    "outputId": "d528b5f3-fbb9-4608-b09d-5e6007f587fd"
   },
   "outputs": [],
   "source": [
    "#Set 2 and 3 EDA - 'total_rech_num_8' can be considered as one of the key predictors. \n",
    "#Churn customer seems to have a very high Roaming calls in the month 8.\n",
    "\n",
    "a, num = plt.subplots(2, 2,figsize=(10,5))\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "\n",
    "box_1 = sns.boxplot(x='churn' ,y='total_rech_num_8',data=df_highvalue ,ax=num[0,0])\n",
    "box_1.set_yscale(\"log\")\n",
    "box_2 = sns.boxplot(x='churn' ,y='total_rech_num_7',data=df_highvalue ,ax=num[0,1])\n",
    "box_2.set_yscale(\"log\")\n",
    "box_3 = sns.boxplot(x='churn' ,y='roam_og_mou_8',data=df_highvalue ,ax=num[1,0])\n",
    "box_3.set_yscale(\"log\")\n",
    "box_4 = sns.boxplot(x='churn' ,y='roam_ic_mou_8',data=df_highvalue ,ax=num[1,1])\n",
    "box_4.set_yscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "T3SWABwAHOaz",
    "outputId": "6e579733-ac1e-41dc-c21b-60bcdc402008"
   },
   "outputs": [],
   "source": [
    "#Set 4-5 EDA - Most Churn customers have not done data recharge in the month 8.\n",
    "#We can check if there is a pattern we can see between recharge of data vs normal recharge for the churn customers\n",
    "#last_day_rch_amt_8 is lower for the Chrun customers in month 8\n",
    "\n",
    "a, num = plt.subplots(1, 3,figsize=(10,5))\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "\n",
    "box_1 = sns.boxplot(x='churn' ,y='vol_2g_mb_8',data=df_highvalue ,ax=num[0])\n",
    "box_1.set_yscale(\"log\")\n",
    "box_2 = sns.boxplot(x='churn' ,y='aug_vbc_3g',data=df_highvalue ,ax=num[1])\n",
    "box_2.set_yscale(\"log\")\n",
    "box_3 = sns.boxplot(x='churn' ,y='last_day_rch_amt_8' ,data=df_highvalue , ax=num[2])\n",
    "box_3.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlCfndpGeL3e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOm4oRSHbkfJ"
   },
   "source": [
    "## Key messages from the above plots\n",
    " \n",
    "\n",
    "1.   Churn Customers are not recharging for data in the month 8.\n",
    "2.   Roaming calls increase for the churn customers in the month 8. this indicates that the customers have moved to other locations and this could be the reason for churn\n",
    "3.   Out going calls for the Churn customers have reduced drastically in the month 8 as compared to 7. This is also related to very less recharge done in the month 8\n",
    "4.   Reduced Local /Total incoming for churners indicating that the customer may already have another mobile in some other network in month 8.\n",
    "\n",
    "## Key recommendation\n",
    "- Relook at the Data plans and Data availability as this seems to be the pain areas for the Customers churning. This is indicated by the reduction in the Data recharge and Value Based Cost - 3G plans in Month 8.\n",
    "- Some customers churn due to they moving out of the existing circle.This is seen with an increase in the Roaming calls for Churn customer. In such case the telecom companies can identify this from the existing data and provide plans accordingly.\n",
    "- Customers do not recharge starting from one month earlier than the churn month(Month 8 in this case). Telecom company can identify these customer and talk to them about the issues they face and provide a solution to the problem they have been facing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mc9FQFRpHAKa"
   },
   "source": [
    "## 5 Perform PCA and identify  a good model for prediction of Churners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "VXmOTORgaPsz",
    "outputId": "2ae7bce1-6f73-4eea-8e4d-58e5560c273e"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pca= PCA(random_state=100)\n",
    "log = LogisticRegression(solver='liblinear' ,class_weight=\"balanced\",penalty='l2',random_state=100)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', log)])\n",
    "param_grid = {\n",
    "    'pca__n_components': [90],\n",
    "    'logistic__C': [0.1,0.05],\n",
    "    'logistic__max_iter' :[500,1000]\n",
    "}\n",
    "grid_log = GridSearchCV(pipe, param_grid ,scoring = 'recall')\n",
    "grid_log.fit(X_train_upsampled_sc, y_train_1)\n",
    "print('-------Logistic regression Best Hyper Params---------------')\n",
    "print(\"Best Recall score=%0.3f:\" % grid_log.best_score_)\n",
    "print(grid_log.best_params_)\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "rfc = RandomForestClassifier(class_weight=\"balanced\",random_state=100)\n",
    "pipe_rfc = Pipeline(steps=[('pca', pca), ('rfc', rfc)])\n",
    "param_rfc = {\n",
    "    'pca__n_components': [50],\n",
    "    'rfc__n_estimators': [100],\n",
    "    'rfc__max_features':[5,10],\n",
    "    'rfc__min_samples_leaf': [5,10]\n",
    "}\n",
    "grid_rfc = GridSearchCV(pipe_rfc, param_rfc ,scoring = 'recall')\n",
    "grid_rfc.fit(X_train_upsampled_sc, y_train_1)\n",
    "print('-------RandomForest Best Hyper Params---------------')\n",
    "print(\"Best Recall score=%0.3f:\" % grid_rfc.best_score_)\n",
    "print(grid_rfc.best_params_)\n",
    "print('-----------------------------------------------------------')\n",
    "\n",
    "svc = SVC(class_weight=\"balanced\",random_state=100)\n",
    "pipe_svc = Pipeline(steps=[('pca', pca), ('svc', svc)])\n",
    "param_grid = {\n",
    "    'pca__n_components': [50],\n",
    "    'svc__C': [0.1],\n",
    "    'svc__kernel':['poly'] ,\n",
    "    'svc__gamma':[0.01]\n",
    "}\n",
    "grid_svc = GridSearchCV(pipe_svc, param_grid ,scoring = 'recall')\n",
    "grid_svc.fit(X_train_upsampled_sc, y_train_1)\n",
    "print('-------SVM Best Hyper Params---------------')\n",
    "print(\"Best Recall score=%0.3f:\" % grid_svc.best_score_)\n",
    "print(grid_svc.best_params_)\n",
    "print('-----------------------------------------------------------')\n",
    "#Best recall score 0.971 in poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "colab_type": "code",
    "id": "aUYnQt1XaPs5",
    "outputId": "ccdf10b9-3be2-4556-a643-19367ab97a0f"
   },
   "outputs": [],
   "source": [
    "#Using the best estimator from the Gird search and using it to predict on the test set\n",
    "\n",
    "grid_best = grid_log.best_estimator_\n",
    "churn_pred = grid_best.predict(X_test_sc)\n",
    "print ('------------ Test data result using best Logistic Model-------------')\n",
    "print (confusion_matrix(y_test,churn_pred,labels=[0,1]))\n",
    "print ('Accuracy score - {0}'.format(accuracy_score(y_test, churn_pred)))\n",
    "print ('F1 score - {0}'.format(f1_score(y_test, churn_pred)))\n",
    "print ('Recall score - {0}'.format(recall_score(y_test, churn_pred)))\n",
    "print ('Precision score - {0}'.format(precision_score(y_test, churn_pred)))\n",
    "print (classification_report(y_test, churn_pred))\n",
    "\n",
    "grid_best = grid_rfc.best_estimator_\n",
    "churn_pred = grid_best.predict(X_test_sc)\n",
    "print ('------------ Test data result using best Random forest Model-------------')\n",
    "print (confusion_matrix(y_test,churn_pred))\n",
    "print ('Accuracy score - {0}'.format(accuracy_score(y_test, churn_pred)))\n",
    "print ('F1 score - {0}'.format(f1_score(y_test, churn_pred)))\n",
    "print ('Recall score - {0}'.format(recall_score(y_test, churn_pred)))\n",
    "print ('Precision score - {0}'.format(precision_score(y_test, churn_pred)))\n",
    "\n",
    "\n",
    "grid_svc = grid_svc.best_estimator_\n",
    "churn_pred = grid_svc.predict(X_test_sc)\n",
    "print ('------------ Test data result using best SVC Model-------------')\n",
    "print (confusion_matrix(y_test,churn_pred))\n",
    "print ('Accuracy score - {0}'.format(accuracy_score(y_test, churn_pred)))\n",
    "print ('F1 score - {0}'.format(f1_score(y_test, churn_pred)))\n",
    "print ('Recall score - {0}'.format(recall_score(y_test, churn_pred)))\n",
    "print ('Precision score - {0}'.format(precision_score(y_test, churn_pred)))\n",
    "print (classification_report(y_test, churn_pred))\n",
    "#Overall Recall score is best for SVM. However the accuracy is very low. \n",
    "#Logistic regression performs much better here as the accuracy and recall score is high.\n",
    "#0.8458 recall score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxqliVfthKMn"
   },
   "source": [
    "## Result\n",
    "- SVM using a poly kernel gives a Recall score of 0.80 on the Test data. This also gives a accuracy of 0.85 on the test dataset. However the Model seems to have some overfitting since there is a hugh difference between the Test and Train score (0.96). \n",
    "- Logistic with l2 penalty gives a Recall score of 0.85 on the Test data and a similar score on the Train data. This also gives a accuracy of 0.83 on the test dataset. There is no overfitting since the scores on Train and Test set are very similar. This seems a good robust model\n",
    "- RandomForest classifier seems ot perform badly on this dataset. It has a good accuracy but a very bad recall score.\n",
    "- Both the Models, SVM and Logistic, are very close but **Logistic gives a good recall and Accuracy and can be used in this case**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Telecom Churn caseStudy-using pipeline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
