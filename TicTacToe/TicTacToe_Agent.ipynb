{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rznSDgbvGggG"
   },
   "source": [
    "## Tic-Tac-Toe Agent\n",
    "â€‹\n",
    "In this notebook, you will learn to build an RL agent (using Q-learning) that learns to play Numerical Tic-Tac-Toe with odd numbers. The environment is playing randomly with the agent, i.e. its strategy is to put an even number randomly in an empty cell. The following is the layout of the notebook:\n",
    "        - Defining epsilon-greedy strategy\n",
    "        - Tracking state-action pairs for convergence\n",
    "        - Define hyperparameters for the Q-learning algorithm\n",
    "        - Generating episode and applying Q-update equation\n",
    "        - Checking convergence in Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eDb8PxBGggH"
   },
   "source": [
    "#### Importing libraries\n",
    "Write the code to import Tic-Tac-Toe class from the environment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SFNYceFGggJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "from TCGame_Env import TicTacToe#- import your class from environment file\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wYLQyopEG8nz"
   },
   "outputs": [],
   "source": [
    "# Function to convert state array into a string to store it as keys in the dictionary\n",
    "# states in Q-dictionary will be of form: x-4-5-3-8-x-x-x-x\n",
    "#   x | 4 | 5\n",
    "#   ----------\n",
    "#   3 | 8 | x\n",
    "#   ----------\n",
    "#   x | x | x\n",
    "\n",
    "def Q_state(state):\n",
    "\n",
    "    return ('-'.join(str(e) for e in state)).replace('nan','x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the pickle files and laod the Remards and Q-values for the states fromt he file\n",
    "#Incase of no file existing , the function will initialize the rewards , Q_dict and States_track dictionaries and return the same.\n",
    "#It also returns a tuple with the boolean value indicating the initialization\n",
    "import os\n",
    "\n",
    "def read_pickle():\n",
    "    policy_flag = False\n",
    "    rewards_flag = False\n",
    "    states_flag = False\n",
    "    \n",
    "    Q_dict = collections.defaultdict(dict)\n",
    "\n",
    "    States_track = collections.defaultdict(dict)\n",
    "\n",
    "    rewards_tracked = {(1,np.nan,np.nan,np.nan,6,np.nan,np.nan,np.nan,np.nan):[],\n",
    "                       (np.nan,np.nan,2,np.nan,7,np.nan,np.nan,np.nan,np.nan):[],\n",
    "                       (np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan):[],\n",
    "                       (np.nan,np.nan,7,3,2,np.nan,np.nan,8,np.nan):[],\n",
    "                       (np.nan,8,np.nan,6,1,3,np.nan,np.nan,np.nan):[],\n",
    "                       (np.nan,8,5,np.nan,9,np.nan,np.nan,np.nan,6):[]}\n",
    "    \n",
    "    if os.path.isfile('./Policy.pkl'):\n",
    "        with open('Policy.pkl', 'rb') as p_handle:\n",
    "            Q_dict = pickle.load(p_handle)\n",
    "            policy_flag = True\n",
    "            \n",
    "    if os.path.isfile('./Rewards.pkl'):\n",
    "        with open('Rewards.pkl', 'rb') as r_handle:\n",
    "            rewards_tracked = pickle.load(r_handle)    \n",
    "            rewards_flag = True\n",
    "    \n",
    "    if os.path.isfile('./States_tracked.pkl'):\n",
    "        with open('States_tracked.pkl', 'rb') as s_handle:\n",
    "            States_track = pickle.load(s_handle)    \n",
    "            states_flag = True\n",
    "\n",
    "    if policy_flag and rewards_flag and states_flag:\n",
    "        return (True ,Q_dict,rewards_tracked,States_track)\n",
    "    else:\n",
    "        return (False,Q_dict,rewards_tracked,States_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZebMOoiVHBBr"
   },
   "outputs": [],
   "source": [
    "# Defining a function which will return valid (all possible actions) actions corresponding to a state\n",
    "# Important to avoid errors during deployment.\n",
    "\n",
    "def valid_actions(state):\n",
    "\n",
    "    valid_Actions = []\n",
    "    \n",
    "    valid_Actions = [i for i in env.action_space(state)[0]] ###### -------please call your environment as env\n",
    "    return valid_Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRciPUkYHDWf"
   },
   "outputs": [],
   "source": [
    "# Defining a function which will add new Q-values to the Q-dictionary. \n",
    "def add_to_dict(state):\n",
    "    state1 = Q_state(state)\n",
    "    \n",
    "    valid_act = valid_actions(state)\n",
    "    if state1 not in Q_dict.keys():\n",
    "        for action in valid_act:\n",
    "            Q_dict[state1][action]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNNi_EfHGggM"
   },
   "source": [
    "#### Epsilon-greedy strategy - Write your code here\n",
    "\n",
    "(you can build your epsilon-decay function similar to the one given at the end of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m0lMfqiJGggN"
   },
   "outputs": [],
   "source": [
    "# Defining epsilon-greedy policy. You can choose any function epsilon-decay strategy\n",
    "def greedy_epsilon(state, time):\n",
    "    max_epsilon = 1.0\n",
    "    min_epsilon = 0.001\n",
    "    #time = np.arange(0,99)\n",
    "    z = np.random.random()\n",
    "    \n",
    "    epsilon =  min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.00005*time)\n",
    "    if z > epsilon:\n",
    "        action = max(Q_dict[Q_state(state)],key=Q_dict[Q_state(state)].get)   #Exploitation: this gets the action corresponding to max q-value of current state\n",
    "    else:\n",
    "        action = np.random.choice(np.arange(0,len (Q_dict[Q_state(state)])))    #Exploration: randomly choosing and action\n",
    "\n",
    "        lst = list(Q_dict[Q_state(state)].items())\n",
    "        action = lst[action]\n",
    "       \n",
    "        pos = str(action).split(\",\")[0][2:]\n",
    "        val = str(action).split(\",\")[1][:-1]\n",
    "        action = [int(pos) ,int(val)]\n",
    "    return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2kyQHOMGggR"
   },
   "source": [
    "#### Tracking the state-action pairs for checking convergence - write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcxZ29vdGggS"
   },
   "outputs": [],
   "source": [
    "#Read the pickle files and load the dictionaries based on what is loaded / initialized\n",
    "initialize = read_pickle()\n",
    "\n",
    "Q_dict = initialize[1]\n",
    "States_track = initialize[3]\n",
    "rewards_tracked = initialize[2]\n",
    "\n",
    "print(len(Q_dict))\n",
    "print(len(rewards_tracked))\n",
    "print(len(States_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vs73iv8fHOxV"
   },
   "outputs": [],
   "source": [
    "# Initialise states to be tracked\n",
    "def initialise_tracking_states():\n",
    "    sample_q_values = [((1,np.nan,np.nan,np.nan,6,np.nan,np.nan,np.nan,np.nan),(1,3)),\n",
    "                       ((np.nan,np.nan,2,np.nan,7,np.nan,np.nan,np.nan,np.nan),(8,3)),\n",
    "                       ((np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan),(5,7)),\n",
    "                       ((np.nan,np.nan,7,3,2,np.nan,np.nan,8,np.nan),(6,5)),\n",
    "                       ((np.nan,8,np.nan,6,1,3,np.nan,np.nan,np.nan),(6,7)),\n",
    "                       ((np.nan,8,5,np.nan,9,np.nan,np.nan,np.nan,6),(6,1))\n",
    "                      ]    #select any 4 Q-values\n",
    "    for q_values in sample_q_values:\n",
    "        state = q_values[0]\n",
    "        action = q_values[1]\n",
    "        States_track[state][action] = []    #this is an array which will have appended values of that state-action pair for every 2000th episode           \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAbwJDMVHpwl"
   },
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Pyj7nMVHsBi"
   },
   "outputs": [],
   "source": [
    "def save_tracking_states():\n",
    "    for state in States_track.keys():\n",
    "        for action in States_track[state].keys():\n",
    "            if Q_state(state) in Q_dict and action in Q_dict[ Q_state(state)]:\n",
    "                   States_track[state][action].append(Q_dict[Q_state(state)][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B_8xSluUHvew"
   },
   "outputs": [],
   "source": [
    "#only needed if the values are not loaded from the pickle files.\n",
    "if not initialize[0]:\n",
    "    initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-iPt--E9GggV"
   },
   "source": [
    "#### Define hyperparameters  ---write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0_f5czFGggW"
   },
   "outputs": [],
   "source": [
    "#Defining parameters for the experiment\n",
    "\n",
    "EPISODES = 100000\n",
    "STEPS = 5\n",
    "LR = 0.01                   #learning rate\n",
    "GAMMA = 0.9\n",
    "\n",
    "threshold = 1000       #every these many episodes, the 4 Q-values will be stored/appended (convergence graphs)\n",
    "policy_threshold = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md6twJ7wGggh"
   },
   "source": [
    "### Q-update loop ---write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldCgQuDNGggj"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "start_time = time.time()\n",
    "#############################################################################\n",
    "for episode in range(EPISODES):\n",
    "    ##### Start writing your code from the next line\n",
    "    if (episode % 100000 == 0):\n",
    "        print ('Episode -' + str(episode))\n",
    "    env = TicTacToe()\n",
    "    \n",
    "    initial_state = env.state \n",
    "    curr_state = env.state\n",
    "    \n",
    "    add_to_dict(curr_state)\n",
    "    \n",
    "    time_step = 0\n",
    "    reward = None\n",
    "\n",
    "    total_reward = 0\n",
    "    \n",
    "    while time_step < STEPS:\n",
    "       \n",
    "        curr_action = greedy_epsilon(curr_state, time_step)\n",
    "\n",
    "        state_reward_term = env.step(copy.deepcopy(curr_state), curr_action)\n",
    "        \n",
    "        next_state = state_reward_term[0]\n",
    "        reward = state_reward_term[1]\n",
    "        terminal= state_reward_term[2]\n",
    "        \n",
    "        tup_curr_action = tuple(curr_action)\n",
    "        if not terminal:\n",
    "            \n",
    "            add_to_dict(next_state)\n",
    "            # For non terminal cases get the action with the action with the max value in the next state\n",
    "            # Use the q-learning equation to assign the value for the current state\n",
    "            try:\n",
    "                max_next = max(Q_dict[Q_state(next_state)],key=Q_dict[Q_state(next_state)].get)   #this gets the action corresponding to max q-value of next state\n",
    "                Q_dict[Q_state(curr_state)][tup_curr_action] += LR * ((reward + (GAMMA*(Q_dict[Q_state(next_state)][tuple(max_next)]))) - Q_dict[Q_state(curr_state)][tup_curr_action] ) \n",
    "            except:\n",
    "                print(next_state)\n",
    "                print (Q_dict[Q_state(next_state)])\n",
    "                break\n",
    "                \n",
    "            total_reward += reward\n",
    "            #TRACKING REWARDS\n",
    "            if tuple(curr_state) in list(rewards_tracked):     #storing rewards\n",
    "                rewards_tracked[tuple(curr_state)].append(total_reward)\n",
    "                \n",
    "            curr_state = next_state       #state(t) became state(t-1)\n",
    "            \n",
    "            time_step += 1\n",
    "    \n",
    "        else:\n",
    "            #In case of terminal state, the state value for the next state is zero and hence the Q-learning equation has been changed a bit to reflect that. \n",
    "            Q_dict[Q_state(curr_state)][tup_curr_action] += LR * (reward - Q_dict[Q_state(curr_state)][tup_curr_action]) \n",
    "            total_reward += reward\n",
    "            #TRACKING REWARDS\n",
    "            if tuple(curr_state) in list(rewards_tracked):     #storing rewards\n",
    "                rewards_tracked[tuple(curr_state)].append(total_reward)\n",
    "            time_step += 1\n",
    "            break\n",
    "\n",
    "      \n",
    "    if ((episode+1) % threshold) == 0:   \n",
    "        save_tracking_states()\n",
    "    \n",
    "################################################################################    \n",
    "elapsed_time = time.time() - start_time\n",
    "save_obj(States_track,'States_tracked')   \n",
    "save_obj(Q_dict,'Policy')\n",
    "save_obj(rewards_tracked,'Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6eMFbb8Ggg2"
   },
   "source": [
    "#### Check the Q-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1tnDJWkGgg9"
   },
   "outputs": [],
   "source": [
    "len(Q_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Key observations -\n",
    "- (1,np.nan,np.nan,np.nan,6,np.nan,np.nan,np.nan,np.nan) - Action (8,9) is the best action and (1,7) is the worst action.\n",
    "- (np.nan,np.nan,2,np.nan,7,np.nan,np.nan,np.nan,np.nan) - Action (6,5) is the best action as suggested by the model.(5,5) is the worst action.\n",
    "- (np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan) - Action (1,1) is the best action and (4,5) is the worst action. \n",
    "- (np.nan,np.nan,7,3,2,np.nan,np.nan,8,np.nan) - Action (1,5) has the highest value as per model and results in a agent win. (5,1) worst options\n",
    "- (np.nan,8,np.nan,6,1,3,np.nan,np.nan,np.nan) - Action (8,5) has the highest value as per model. \n",
    "- (np.nan,8,5,np.nan,9,np.nan,np.nan,np.nan,6) - Action (6,1) has the highest value as per model and results in a agent win.(0,1) is the worst option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFgUqfcQGghB"
   },
   "outputs": [],
   "source": [
    "# try checking for one of the states - that which action your agent thinks is the best  -----This will not be evaluated\n",
    "initialize = read_pickle()   #Read the pickle files for rewards , qdict and state_track\n",
    "\n",
    "Q_dict = initialize[1]\n",
    "States_track = initialize[3]\n",
    "rewards_tracked = initialize[2]\n",
    "\n",
    "for state in States_track.keys():\n",
    "    print (state)\n",
    "    print (Q_dict[Q_state(state)])\n",
    "    print ('-------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KGPZEQDFGghG"
   },
   "source": [
    "#### Check the states tracked for Q-values convergence\n",
    "(non-evaluative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVQInsg7GghL"
   },
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "fig,axs = plt.subplots(3,2,figsize=(15,15))\n",
    "for state in States_track.keys():\n",
    "    for action in States_track[state].keys():\n",
    "        axs[i,j].set_title('state='+str(state)+' action='+str(action))\n",
    "        axs[i,j].plot (np.asarray(States_track[state][action]))\n",
    "        if (i==0 and j == 0):\n",
    "            j += 1\n",
    "        elif (i==0 and j==1):\n",
    "            i += 1\n",
    "            j = 0\n",
    "        elif (i==1 and j==0):\n",
    "            j += 1\n",
    "        elif (i==1 and j==1):\n",
    "            i += 1\n",
    "            j = 0\n",
    "        else:\n",
    "            j += 1                   \n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2Opp8_NITkC"
   },
   "source": [
    "### Epsilon - decay check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQ_D_JsuGghR"
   },
   "outputs": [],
   "source": [
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "time = np.arange(0,100000)\n",
    "epsilon = []\n",
    "for i in range(0,100000):\n",
    "    epsilon.append(min_epsilon + (max_epsilon - min_epsilon) * np.exp(-0.00005*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "J7c2xADQGghV",
    "outputId": "cb60fce3-570b-45fb-bd83-abde3d13b273"
   },
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "59BRf43IJiQ1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TicTacToe_Agent.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
