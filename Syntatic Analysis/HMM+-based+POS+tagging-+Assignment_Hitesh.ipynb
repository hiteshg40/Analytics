{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging using modified Viterbi\n",
    "\n",
    "1. Data Preparation\n",
    "2. Perform EDA to identify the pattern in the train set\n",
    "3. Build the vanilla Viterbi based POS tagger    \n",
    "    1. Evaluate the tagging acuracy with the validation set\n",
    "4. Solve the problem of unknown words\n",
    "    1. Technique 1 - Using a Rule tagger with the vanilla Viterbi algorithm\n",
    "        1. Evaluating tagging accuracy for the algorithm using the rule tagger with simple vitrebi agorithm on the validation set\n",
    "    2. Technique 2 - Using the transition probability for tagging when the emission probability is zero\n",
    "        1. Evaluating tagging accuracy for the algorithm using transition probability on the validation set\n",
    "\n",
    "5. Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm\n",
    "6. Find accuracy on the test set for the different Techniques used\n",
    "    1.  Accuracy of simple Vitrebi on the test set\n",
    "    2.  Accuracy of Rule tagger + simple Vitrebi on the test set\n",
    "    3.  Accuracy of Transition probaility + simple Vitrebi on the test set\n",
    "\n",
    "7.  Comparision of the Incorrect words across different algorithms     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\uvoc6q\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "nltk.download('universal_tagset')\n",
    "nltk_data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (nltk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "#distribute the treebank set into train and validation sets\n",
    "train_set, validation_set = train_test_split(nltk_data,train_size=0.95 ,random_state=100)\n",
    "print (len (train_set))\n",
    "print (len(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function to extract the indiviual words in the dataset along with the tags\n",
    "#get the form [(w1,t1,(w2,t2))]\n",
    "def word_pos_tuple(dataset=train_set):\n",
    "    word_pos = [tup for sent in dataset for tup in sent]\n",
    "    return word_pos\n",
    "\n",
    "#Write the function to get the word  from the dataset\n",
    "def word_token(dataset=train_set):\n",
    "    words = [tup[0] for sent in dataset for tup in sent]\n",
    "    return words\n",
    "\n",
    "def all_tags(dataset=train_set):\n",
    "    word_pos = [tup[1] for sent in dataset for tup in sent]\n",
    "    return word_pos\n",
    "\n",
    "def to_sentence(dataset=train_set):\n",
    "    sentence = []\n",
    "    for sent in dataset:\n",
    "        sentence.append(\" \".join([tup[0] for tup in sent]))\n",
    "    return sentence\n",
    "\n",
    "\n",
    "#write functions to get the emission and transaction probability for the token\n",
    "#Emission probability is (count of word marked as tag / total instance of the tag)\n",
    "def word_emission(word, tag, dataset):\n",
    "    word_with_tag = [tup for tup in dataset if (tup[0] == word and tup[1]== tag)]\n",
    "    count_word_with_tag = len (word_with_tag)\n",
    "    \n",
    "    total_tag_inst = len([tup[1] for tup in dataset if tup[1] == tag])\n",
    "    \n",
    "    return (count_word_with_tag , total_tag_inst ) \n",
    "\n",
    "#calculate the transition probability\n",
    "def t2_given_t1 (t2,t1,dataset):\n",
    "    all_train_tag = [tup[1] for tup in dataset]\n",
    "    count_t1 = len([tag for tag in all_train_tag if tag == t1])\n",
    "    count_t1_t2 = 0\n",
    "    for i in range (0,len(all_train_tag)-1):\n",
    "        if (all_train_tag[i] == t1 and all_train_tag[i+1] == t2):\n",
    "            count_t1_t2 += 1\n",
    "\n",
    "    return (count_t1_t2 ,count_t1)                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the word-pos tuple for the train and validation set\n",
    "train_word_tup = word_pos_tuple (train_set)\n",
    "validation_word_tup = word_pos_tuple (validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the word and token list\n",
    "train_words = word_token(train_set)\n",
    "train_tags = set(all_tags(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe with the transition probabilities for the tags\n",
    "\n",
    "tags_matrix = np.zeros((len(train_tags), len(train_tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(train_tags)):\n",
    "    for j, t2 in enumerate(list(train_tags)):\n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1,train_word_tup)[0]/t2_given_t1(t2, t1,train_word_tup)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PRON</th>\n",
       "      <th>PRT</th>\n",
       "      <th>DET</th>\n",
       "      <th>X</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NUM</th>\n",
       "      <th>ADV</th>\n",
       "      <th>.</th>\n",
       "      <th>ADP</th>\n",
       "      <th>NOUN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.067158</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.010156</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.020311</td>\n",
       "      <td>0.016052</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>0.020803</td>\n",
       "      <td>0.004914</td>\n",
       "      <td>0.063882</td>\n",
       "      <td>0.078624</td>\n",
       "      <td>0.700901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.072031</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>0.009195</td>\n",
       "      <td>0.092720</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.484291</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.040613</td>\n",
       "      <td>0.023372</td>\n",
       "      <td>0.211494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.083661</td>\n",
       "      <td>0.017717</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.101050</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.405184</td>\n",
       "      <td>0.056102</td>\n",
       "      <td>0.010171</td>\n",
       "      <td>0.043635</td>\n",
       "      <td>0.019357</td>\n",
       "      <td>0.245735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.204977</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>0.045323</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.040394</td>\n",
       "      <td>0.021640</td>\n",
       "      <td>0.012623</td>\n",
       "      <td>0.017913</td>\n",
       "      <td>0.009618</td>\n",
       "      <td>0.637293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.016505</td>\n",
       "      <td>0.055705</td>\n",
       "      <td>0.184891</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>0.074433</td>\n",
       "      <td>0.010316</td>\n",
       "      <td>0.204571</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.162831</td>\n",
       "      <td>0.144898</td>\n",
       "      <td>0.062371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.118683</td>\n",
       "      <td>0.058414</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.118683</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.155308</td>\n",
       "      <td>0.042188</td>\n",
       "      <td>0.053778</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>0.053778</td>\n",
       "      <td>0.350487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.065640</td>\n",
       "      <td>0.035916</td>\n",
       "      <td>0.031427</td>\n",
       "      <td>0.133292</td>\n",
       "      <td>0.218438</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.168744</td>\n",
       "      <td>0.022448</td>\n",
       "      <td>0.082050</td>\n",
       "      <td>0.034291</td>\n",
       "      <td>0.091184</td>\n",
       "      <td>0.111386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.033571</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.026144</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.211824</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.184195</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.118835</td>\n",
       "      <td>0.035056</td>\n",
       "      <td>0.352347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.130160</td>\n",
       "      <td>0.015646</td>\n",
       "      <td>0.014314</td>\n",
       "      <td>0.069907</td>\n",
       "      <td>0.023302</td>\n",
       "      <td>0.006991</td>\n",
       "      <td>0.344541</td>\n",
       "      <td>0.031624</td>\n",
       "      <td>0.077230</td>\n",
       "      <td>0.135153</td>\n",
       "      <td>0.119507</td>\n",
       "      <td>0.031624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.043681</td>\n",
       "      <td>0.065208</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.173558</td>\n",
       "      <td>0.026908</td>\n",
       "      <td>0.058032</td>\n",
       "      <td>0.088708</td>\n",
       "      <td>0.081353</td>\n",
       "      <td>0.052292</td>\n",
       "      <td>0.092923</td>\n",
       "      <td>0.092206</td>\n",
       "      <td>0.222531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.107389</td>\n",
       "      <td>0.069119</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.323969</td>\n",
       "      <td>0.034984</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.008481</td>\n",
       "      <td>0.061910</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.039754</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.321213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.012165</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.043357</td>\n",
       "      <td>0.013363</td>\n",
       "      <td>0.028868</td>\n",
       "      <td>0.042921</td>\n",
       "      <td>0.146955</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>0.239951</td>\n",
       "      <td>0.177058</td>\n",
       "      <td>0.264280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ADJ      PRON       PRT       DET         X      CONJ      VERB  \\\n",
       "ADJ   0.067158  0.000491  0.010156  0.004914  0.020311  0.016052  0.011794   \n",
       "PRON  0.072031  0.007663  0.012261  0.009195  0.092720  0.004981  0.484291   \n",
       "PRT   0.083661  0.017717  0.001969  0.101050  0.013123  0.002297  0.405184   \n",
       "DET   0.204977  0.003727  0.000240  0.005771  0.045323  0.000481  0.040394   \n",
       "X     0.016505  0.055705  0.184891  0.055229  0.074433  0.010316  0.204571   \n",
       "CONJ  0.118683  0.058414  0.003709  0.118683  0.008809  0.000464  0.155308   \n",
       "VERB  0.065640  0.035916  0.031427  0.133292  0.218438  0.005186  0.168744   \n",
       "NUM   0.033571  0.001485  0.026144  0.003862  0.211824  0.013072  0.016934   \n",
       "ADV   0.130160  0.015646  0.014314  0.069907  0.023302  0.006991  0.344541   \n",
       ".     0.043681  0.065208  0.002511  0.173558  0.026908  0.058032  0.088708   \n",
       "ADP   0.107389  0.069119  0.001484  0.323969  0.034984  0.000848  0.008481   \n",
       "NOUN  0.012165  0.004721  0.043357  0.013363  0.028868  0.042921  0.146955   \n",
       "\n",
       "           NUM       ADV         .       ADP      NOUN  \n",
       "ADJ   0.020803  0.004914  0.063882  0.078624  0.700901  \n",
       "PRON  0.007280  0.034100  0.040613  0.023372  0.211494  \n",
       "PRT   0.056102  0.010171  0.043635  0.019357  0.245735  \n",
       "DET   0.021640  0.012623  0.017913  0.009618  0.637293  \n",
       "X     0.002857  0.025393  0.162831  0.144898  0.062371  \n",
       "CONJ  0.042188  0.053778  0.035698  0.053778  0.350487  \n",
       "VERB  0.022448  0.082050  0.034291  0.091184  0.111386  \n",
       "NUM   0.184195  0.002674  0.118835  0.035056  0.352347  \n",
       "ADV   0.031624  0.077230  0.135153  0.119507  0.031624  \n",
       ".     0.081353  0.052292  0.092923  0.092206  0.222531  \n",
       "ADP   0.061910  0.013357  0.039754  0.017492  0.321213  \n",
       "NOUN  0.009550  0.016813  0.239951  0.177058  0.264280  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = train_tags, index=train_tags)\n",
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# heatmap of tags matrix\n",
    "# T(i, j) means P(tag j given tag i)\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(tags_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perform EDA to identify the pattern in the train set\n",
    "Look for the most common tag and the pattern across each tag\n",
    "- 35% of Nouns are preceded by a Det or an adjective\n",
    "- 30% of verbs end with 'ed' or 'ing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 27539, 'VERB': 12919, '.': 11149, 'ADP': 9433, 'DET': 8318, 'X': 6301, 'ADJ': 6105, 'NUM': 3366, 'PRT': 3048, 'ADV': 3004, 'PRON': 2610, 'CONJ': 2157})\n",
      "95949\n",
      "Percent of words ending with ed in tags marked as VERB is 20.61%\n",
      "Percent of words ending with ing in tags marked as VERB is 10.97%\n",
      "Number of instance a ADV is followed by Verb is 8.01%\n",
      "Number of instance a ADJ is followed by Noun is 15.54%\n",
      "Number of instance the start of the sentence is Noun is 3.62%\n",
      "Number of instance a DET is followed by Noun is 19.25%\n"
     ]
    }
   ],
   "source": [
    "#Check for the most common tag across the training dataset\n",
    "from collections import Counter\n",
    "all_t = all_tags()\n",
    "print (Counter(all_t))       #Noun seems to be the most common tag\n",
    "\n",
    "word_tup = word_pos_tuple()\n",
    "print (len(word_tup))             #Total word-tags is 99637\n",
    "\n",
    "tag_verb = [tag for tag in word_tup if tag[1] == 'VERB']\n",
    "tag_verb_ed = [tag[0] for tag in tag_verb if tag[0].lower().endswith('ed')]\n",
    "print (\"Percent of words ending with ed in tags marked as VERB is {:.2%}\".format(len(tag_verb_ed) / len(tag_verb)))\n",
    "\n",
    "tag_verb_ing = [tag[0] for tag in tag_verb if tag[0].lower().endswith('ing')]\n",
    "print (\"Percent of words ending with ing in tags marked as VERB is {:.2%}\".format(len(tag_verb_ing) / len(tag_verb)))\n",
    "\n",
    "tag_adv_verb = [tag for i,tag in enumerate(all_t) if (tag == 'VERB' and all_t[i-1] == 'ADV')] \n",
    "print (\"Number of instance a ADV is followed by Verb is {:.2%}\".format(len(tag_adv_verb) / len(tag_verb)))\n",
    "\n",
    "#adjectives followed by noun\n",
    "tag_noun = [tag for tag in word_tup if tag[1] == 'NOUN']\n",
    "tag_adj_noun = [tag for i,tag in enumerate(all_t) if (tag == 'NOUN' and all_t[i-1] == 'ADJ')]  \n",
    "print (\"Number of instance a ADJ is followed by Noun is {:.2%}\".format(len(tag_adj_noun) / len(tag_noun)))\n",
    "\n",
    "#. followed by Noun\n",
    "tag_X_noun = [tag for i,tag in enumerate(word_tup) if (tag[1] == 'NOUN' and word_tup[i-1][0] == '.')]\n",
    "print (\"Number of instance the start of the sentence is Noun is {:.2%}\".format(len(tag_X_noun) / len(tag_noun)))\n",
    "\n",
    "#det followed by noun\n",
    "tag_det_noun = [tag for i,tag in enumerate(all_t) if (tag == 'NOUN' and all_t[i-1] == 'DET')]  \n",
    "print (\"Number of instance a DET is followed by Noun is {:.2%}\".format(len(tag_det_noun) / len(tag_noun)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the vanilla Viterbi based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_viterbi(sent ,dataset=train_word_tup):\n",
    "    #For each word in the sentence get the probaility for each unique tage and \n",
    "    #then choose the tag with the max probability\n",
    "   \n",
    "    unique_tags = list(set(tup[1] for tup in dataset))\n",
    "    \n",
    "    sent_tag = []\n",
    "    prob_list = []\n",
    "    for i, word in enumerate(sent):\n",
    "        p_total = [] \n",
    "        for tag in unique_tags:\n",
    "            if i == 0 :\n",
    "                tran_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                tran_p = tags_df.loc[sent_tag[i-1], tag]\n",
    "                \n",
    "            emission_p = word_emission(word, tag,train_word_tup)[0]/word_emission(word, tag, train_word_tup)[1]\n",
    "            p_total.append(emission_p * tran_p)\n",
    "\n",
    "        #get max probability and corresponding tag\n",
    "        p_vit = max(p_total)\n",
    "        prob_list.append(p_vit)\n",
    "        index = p_total.index(max(p_total))\n",
    "        sent_tag.append (unique_tags[index])\n",
    "        \n",
    "    return list(zip(sent, sent_tag , prob_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Evaluate the tagging acuracy with the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tagged words\n",
    "validation_tup = word_pos_tuple(validation_set)\n",
    "\n",
    "# list of untagged words\n",
    "validation_untagged = word_token(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  1850.6993198394775\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "validation_predicted = simple_viterbi(validation_untagged,train_word_tup)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4322\n",
      "Accuracy for this model is 91.43%\n"
     ]
    }
   ],
   "source": [
    "#Get the word and tag from the predicted result\n",
    "tagged_seq = [(tup[0],tup[1]) for tup in validation_predicted]\n",
    "\n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, validation_word_tup) if i == j] \n",
    "print (len(check))\n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print (\"Accuracy for this model is {:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words which were tagged incorrectly by the algorithm -  405\n"
     ]
    }
   ],
   "source": [
    "#identify the incorrect words\n",
    "incorrect_tagged_cases = [[validation_word_tup[i-1],j] for i, j in \\\n",
    "                          enumerate(zip(tagged_seq, validation_word_tup)) if j[0]!=j[1]]\n",
    "print (\"Number of words which were tagged incorrectly by the algorithm - \",len(incorrect_tagged_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unknown words (0 emission probability) -  309\n"
     ]
    }
   ],
   "source": [
    "#Identify words with 0 probability\n",
    "print (\"Number of Unknown words (0 emission probability) - \",len([tup for tup in validation_predicted if tup[2] == 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[tup for tup in validation_predicted if tup[2] == 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Solve the problem of unknown words\n",
    "\n",
    "It is seen that most of the words which are incorrectly tagged is due to words not found in the training set. As a result the emission probability is 0. Hence the function picks the first tag as the appropriate tag.\n",
    "\n",
    "- This can be corrected by adding additional rules based tagger for the words where the emission probability is 0.This has been done in the next section\n",
    "- Another way to resolves this is by considering the transition probability when the emission probability is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a Using a Rule tagger with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same vanilla viterbi function has been used along with a Rule tagger to resolve the problem of unknown words.\n",
    "def ruletagger_viterbi(sent ,dataset=train_word_tup):\n",
    "    #For each word in the sentence get the probaility for each unique tage and \n",
    "    #then choose the tag with the max probability\n",
    "   \n",
    "    unique_tags = list(set(tup[1] for tup in dataset))\n",
    "    sent_tag = []\n",
    "    prob_list = []\n",
    "    for i, word in enumerate(sent):\n",
    "        p_total = [] \n",
    "        for tag in unique_tags:\n",
    "            if i == 0 :\n",
    "                tran_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                tran_p = tags_df.loc[sent_tag[i-1], tag]\n",
    "                \n",
    "            emission_p = word_emission(word, tag,train_word_tup)[0]/word_emission(word, tag, train_word_tup)[1]\n",
    "            p_total.append(emission_p * tran_p)\n",
    "\n",
    "        #get max probability and corresponding tag\n",
    "        p_vit = max(p_total)\n",
    "        prob_list.append(p_vit)\n",
    "        if p_vit == 0.0:\n",
    "            #For the words where the probability (emission) is 0, use the rule tagger for that word\n",
    "            #Probability is still kept as 0 to identify such words which have gone through the rule tagger\n",
    "            sent_tag.append(rule_tagger([word])[0][1]) \n",
    "        else:\n",
    "            index = p_total.index(max(p_total))\n",
    "            sent_tag.append (unique_tags[index])\n",
    "    return list(zip(sent, sent_tag , prob_list))\n",
    "\n",
    "#Use a rule based tagger for the words which cannot be tagged by the simple viterbi model\n",
    "def rule_tagger(word ,dataset=train_word_tup):\n",
    "    patterns = [\n",
    "        (r'.*ing$', 'VERB'),          #words ending with ing    \n",
    "        (r'.*ed$', 'VERB'),           #words ending with ed\n",
    "        (r'\\d+(\\.\\d*)?$', 'NUM'),\n",
    "        #(r'[0-9]+$', 'NUM'),         #words which are number\n",
    "        (r'.*[0-9]+.*', 'X'),         #words which have text, symbol  and numbers are classified as X\n",
    "        (r'[A-Z]{1,4}$','X'),         #words which are abbrevition upto 4 chars\n",
    "        (r'.*st$', 'ADJ'),            #words ending with st clasified as ADJ\n",
    "        (r'.*', 'NOUN')               #All others are classified as noun     \n",
    "    ]\n",
    "    regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "    return regexp_tagger.tag(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.a.1 Evaluating tagging accuracy for the algorithm using the rule tagger with simple vitrebi agorithm on the validation set\n",
    "Evaluate the tagging accuracy for the modified algorithm on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  1664.9537239074707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "validation_predicted_2 = ruletagger_viterbi(validation_untagged,train_word_tup)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this model is 94.92%\n",
      "Number of words which were tagged incorrectly by this algorithm -  240\n"
     ]
    }
   ],
   "source": [
    "#Get the word and tag from the predicted result\n",
    "tagged_seq_2 = [(tup[0],tup[1]) for tup in validation_predicted_2]\n",
    "\n",
    "# accuracy\n",
    "check_2 = [i for i, j in zip(tagged_seq_2, validation_word_tup) if i == j] \n",
    "#print (len(check_2))\n",
    "accuracy_2 = len(check_2)/len(tagged_seq_2)\n",
    "print (\"Accuracy for this model is {:.2%}\".format(accuracy_2))\n",
    "\n",
    "#identify the incorrect words\n",
    "incorrect_tagged_cases = [[validation_word_tup[i-1],j] for i, j in \\\n",
    "                          enumerate(zip(tagged_seq_2, validation_word_tup)) if j[0]!=j[1]]\n",
    "print ('Number of words which were tagged incorrectly by this algorithm - ',len(incorrect_tagged_cases))\n",
    "\n",
    "#Identify words with 0 probability\n",
    "#print ([tup for tup in validation_predicted_2 if tup[2] == 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incorrect_tagged_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b Using only the transition probability for tagging when the emission probability is zero. using this with the vanilla Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transitionP_viterbi(sent ,dataset=train_word_tup):\n",
    "    #For each word in the sentence get the probaility for each unique tage and \n",
    "    #then choose the tag with the max probability\n",
    "    #in case the emission probability is zero then only use the transition probability for such cases\n",
    "   \n",
    "    unique_tags = list(set(tup[1] for tup in dataset))\n",
    "    sent_tag = []\n",
    "    prob_list = []\n",
    "    \n",
    "    for i, word in enumerate(sent):\n",
    "        p_total = [] \n",
    "        lst_ptrans = []\n",
    "        for tag in unique_tags:\n",
    "            if i == 0 :\n",
    "                tran_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                tran_p = tags_df.loc[sent_tag[i-1], tag]\n",
    "                \n",
    "            emission_p = word_emission(word, tag,train_word_tup)[0]/word_emission(word, tag, train_word_tup)[1]\n",
    "            p_total.append(emission_p * tran_p)\n",
    "            lst_ptrans.append(tran_p)\n",
    "        #get max probability and corresponding tag\n",
    "        p_vit = max(p_total)\n",
    "        p_trans = max(lst_ptrans)\n",
    "        prob_list.append(p_vit) # populate this to 0 to identify the words which used only Transition Probability\n",
    "        if (p_vit == 0.0 and p_trans > 0.0):\n",
    "            #find the tag index with max Trans probability\n",
    "            index = lst_ptrans.index(p_trans)\n",
    "        else:            \n",
    "            index = p_total.index(max(p_total))\n",
    "        sent_tag.append (unique_tags[index])\n",
    "    return list(zip(sent, sent_tag , prob_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.b.1 Evaluating tagging accuracy for the algorithm using transition probability (when emission probability is not available)on the validation set\n",
    "Evaluate the tagging accuracy for the modified algorithm on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  1649.9899909496307\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "validation_predicted_3 = transitionP_viterbi(validation_untagged,train_word_tup)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    "\n",
    "print(\"Time taken in seconds: \", difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this model is 93.31%\n",
      "Number of words which were tagged incorrectly by this algorithm -  316\n"
     ]
    }
   ],
   "source": [
    "#Get the word and tag from the predicted result\n",
    "tagged_seq_3 = [(tup[0],tup[1]) for tup in validation_predicted_3]\n",
    "\n",
    "# accuracy\n",
    "check_3 = [i for i, j in zip(tagged_seq_3, validation_word_tup) if i == j] \n",
    "#print (len(check_2))\n",
    "accuracy_3 = len(check_3)/len(tagged_seq_3)\n",
    "print (\"Accuracy for this model is {:.2%}\".format(accuracy_3))\n",
    "\n",
    "#identify the incorrect words\n",
    "incorrect_tagged_cases = [[validation_word_tup[i-1],j] for i, j in \\\n",
    "                          enumerate(zip(tagged_seq_3, validation_word_tup)) if j[0]!=j[1]]\n",
    "print ('Number of words which were tagged incorrectly by this algorithm - ',len(incorrect_tagged_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.c.1 Evaluating tagging accuracy for the algorithm using the Lexicon tagger with simple vitrebi agorithm on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compare the tagging accuracies of the modifications with the vanilla Viterbi algorithm on the validation set\n",
    "\n",
    "As seen from the accuracies for the validation set when the transition probability table does not start with NOUN - \n",
    "- Vanilla Vitrebi Algorithm - ~90% accuracy - ~449 tagged incorrectly\n",
    "\n",
    "However this change when the transition probability table starts with NOUN -\n",
    "- Vanilla Vitrebi Algorithm - ~94% accuracy\n",
    "\n",
    "Accuracy of the modified Vitrebi \n",
    "- Vanilla Vitrebi + Rule tagger - ~95%\n",
    "- Vanilla Vitrebi + Using Transition probability when Emission probability is 0 - ~93.35% \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Find accuracy on the test set for the different Techniques used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the function to check tagging for the test sentences\n",
    "data = pd.read_csv('Test_sentences.txt',  delimiter = \"\\t\", header=None)\n",
    "\n",
    "#tokenize these sentennces and form a list\n",
    "test_untagged = []\n",
    "for i in range(0,len(data)):\n",
    "    test_untagged.append(word_tokenize(data.iloc[i,0]))\n",
    "\n",
    "word_list = [wd for sent in test_untagged for wd in sent]\n",
    "\n",
    "SimpleVit_pred = simple_viterbi(word_list,train_word_tup)\n",
    "Rule_SimpleVit_pred = ruletagger_viterbi(word_list,train_word_tup)\n",
    "TransP_SimpleVit_pred = transitionP_viterbi(word_list,train_word_tup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the POS tags to the test data.\n",
    "#https://parts-of-speech.info/ used to assign the 9 POStags available there\n",
    "#This is then modified manually for the missing tags such as X,PRT and Puncuation\n",
    "test_pos = [('Android','NOUN'),\n",
    " ('is','VERB'),\n",
    " ('a','DET'),\n",
    " ('mobile','ADJ'),\n",
    " ('operating','NOUN'),\n",
    " ('system','NOUN'),\n",
    " ('developed','VERB'),\n",
    " ('by','ADP'),\n",
    " ('Google','NOUN'),\n",
    " ('.','.'),\n",
    " ('Android','NOUN'),\n",
    " ('has','VERB'),\n",
    " ('been','VERB'),\n",
    " ('the','DET'),\n",
    " ('best-selling','ADJ'),\n",
    " ('OS','X'),\n",
    " ('worldwide','NOUN'),\n",
    " ('on','ADP'),\n",
    " ('smartphones','NOUN'),\n",
    " ('since','ADP'),\n",
    " ('2011','NUM'),\n",
    " ('and','CONJ'),\n",
    " ('on','ADP'),\n",
    " ('tablets','NOUN'),\n",
    " ('since','ADP'),\n",
    " ('2013','NUM'),\n",
    " ('.','.'),\n",
    " ('Google','NOUN'),\n",
    " ('and','CONJ'),\n",
    " ('Twitter','NOUN'),\n",
    " ('made','VERB'),\n",
    " ('a','DET'),\n",
    " ('deal','NOUN'),\n",
    " ('in','ADP'),\n",
    " ('2015','NUM'),\n",
    " ('that','DET'),\n",
    " ('gave','VERB'),\n",
    " ('Google','NOUN'),\n",
    " ('access','NOUN'),\n",
    " ('to','PRT'),\n",
    " ('Twitter','NOUN'),\n",
    " (\"'s\",'PRT'),\n",
    " ('firehose','NOUN'),\n",
    " ('.','.'),\n",
    " ('Twitter','NOUN'),\n",
    " ('is','VERB'),\n",
    " ('an','DET'),\n",
    " ('online','ADJ'),\n",
    " ('news','NOUN'),\n",
    " ('and','CONJ'),\n",
    " ('social','ADJ'),\n",
    " ('networking','NOUN'),\n",
    " ('service','NOUN'),\n",
    " ('on','ADP'),\n",
    " ('which','DET'),\n",
    " ('users','NOUN'),\n",
    " ('post','VERB'),\n",
    " ('and','CONJ'),\n",
    " ('interact','VERB'),\n",
    " ('with','ADP'),\n",
    " ('messages','NOUN'),\n",
    " ('known','VERB'),\n",
    " ('as','ADP'),\n",
    " ('tweets','NOUN'),\n",
    " ('.','.'),\n",
    " ('Before','ADP'),\n",
    " ('entering','VERB'),\n",
    " ('politics','NOUN'),\n",
    " (',','.'),\n",
    " ('Donald','NOUN'),\n",
    " ('Trump','NOUN'),\n",
    " ('was','VERB'),\n",
    " ('a','DET'),\n",
    " ('domineering','ADJ'),\n",
    " ('businessman','NOUN'),\n",
    " ('and','CONJ'),\n",
    " ('a','DET'),\n",
    " ('television','NOUN'),\n",
    " ('personality','NOUN'),\n",
    " ('.','.'),\n",
    " ('The','DET'),\n",
    " ('2018','NUM'),\n",
    " ('FIFA','X'),\n",
    " ('World','NOUN'),\n",
    " ('Cup','NOUN'),\n",
    " ('is','VERB'),\n",
    " ('the','DET'),\n",
    " ('21st','NUM'),\n",
    " ('FIFA','X'),\n",
    " ('World','NOUN'),\n",
    " ('Cup','NOUN'),\n",
    " (',','.'),\n",
    " ('an','DET'),\n",
    " ('international','ADJ'),\n",
    " ('football','NOUN'),\n",
    " ('tournament','NOUN'),\n",
    " ('contested','VERB'),\n",
    " ('once','ADV'),\n",
    " ('every','DET'),\n",
    " ('four','NUM'),\n",
    " ('years','NOUN'),\n",
    " ('.','.'),\n",
    " ('This','DET'),\n",
    " ('is','VERB'),\n",
    " ('the','DET'),\n",
    " ('first','ADJ'),\n",
    " ('World','NOUN'),\n",
    " ('Cup','NOUN'),\n",
    " ('to','PRT'),\n",
    " ('be','VERB'),\n",
    " ('held','VERB'),\n",
    " ('in','ADP'),\n",
    " ('Eastern','NOUN'),\n",
    " ('Europe','NOUN'),\n",
    " ('and','CONJ'),\n",
    " ('the','DET'),\n",
    " ('11th','ADJ'),\n",
    " ('time','NOUN'),\n",
    " ('that','ADP'),\n",
    " ('it','PRON'),\n",
    " ('has','VERB'),\n",
    " ('been','VERB'),\n",
    " ('held','VERB'),\n",
    " ('in','ADP'),\n",
    " ('Europe','NOUN'),\n",
    " ('.','.'),\n",
    " ('Show','NOUN'),\n",
    " ('me','PRON'),\n",
    " ('the','DET'),\n",
    " ('cheapest','ADJ'),\n",
    " ('round','NOUN'),\n",
    " ('trips','NOUN'),\n",
    " ('from','ADP'),\n",
    " ('Dallas','NOUN'),\n",
    " ('to','PRT'),\n",
    " ('Atlanta','NOUN'),\n",
    " ('I','PRON'),\n",
    " ('would','VERB'),\n",
    " ('like','VERB'),\n",
    " ('to','PRT'),\n",
    " ('see','VERB'),\n",
    " ('flights','NOUN'),\n",
    " ('from','ADP'),\n",
    " ('Denver','NOUN'),\n",
    " ('to','PRT'),\n",
    " ('Philadelphia','NOUN'),\n",
    " ('.','.'),\n",
    " ('Show','NOUN'),\n",
    " ('me','PRON'),\n",
    " ('the','DET'),\n",
    " ('price','NOUN'),\n",
    " ('of','ADP'),\n",
    " ('the','DET'),\n",
    " ('flights','NOUN'),\n",
    " ('leaving','VERB'),\n",
    " ('Atlanta','NOUN'),\n",
    " ('at','ADP'),\n",
    " ('about','ADP'),\n",
    " ('3','NUM'),\n",
    " ('in','ADP'),\n",
    " ('the','DET'),\n",
    " ('afternoon','NOUN'),\n",
    " ('and','CONJ'),\n",
    " ('arriving','VERB'),\n",
    " ('in','ADP'),\n",
    " ('San','NOUN'),\n",
    " ('Francisco','NOUN'),\n",
    " ('.','.'),\n",
    " ('NASA','X'),\n",
    " ('invited','VERB'),\n",
    " ('social','ADJ'),\n",
    " ('media','NOUN'),\n",
    " ('users','NOUN'),\n",
    " ('to','PRT'),\n",
    " ('experience','VERB'),\n",
    " ('the','DET'),\n",
    " ('launch','NOUN'),\n",
    " ('of','ADP'),\n",
    " ('ICESAT-2','X'),\n",
    " ('Satellite','NOUN'),\n",
    " ('.','.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of simple Vitrebi on the test set\n",
    "- Accuracy for this model is 77%\n",
    "- Number of words which were tagged incorrectly by Simple Vitrebi algorithm -  40\n",
    "\n",
    "#### the accuracy will be high around 85% if  'NOUN' has been selected as the first tag. For other tags the accuracy will fall to almost 77%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this model is 78.45%\n",
      "Number of words which were tagged incorrectly by Simple Vitrebi algorithm -  39\n",
      "[[('.', '.'), (('Android', 'ADJ'), ('Android', 'NOUN'))], [('by', 'ADP'), (('Google', 'ADJ'), ('Google', 'NOUN'))], [('.', '.'), (('Android', 'ADJ'), ('Android', 'NOUN'))], [('best-selling', 'ADJ'), (('OS', 'ADJ'), ('OS', 'X'))], [('OS', 'X'), (('worldwide', 'ADJ'), ('worldwide', 'NOUN'))], [('on', 'ADP'), (('smartphones', 'ADJ'), ('smartphones', 'NOUN'))], [('since', 'ADP'), (('2011', 'ADJ'), ('2011', 'NUM'))], [('since', 'ADP'), (('2013', 'ADJ'), ('2013', 'NUM'))], [('.', '.'), (('Google', 'ADJ'), ('Google', 'NOUN'))], [('and', 'CONJ'), (('Twitter', 'ADJ'), ('Twitter', 'NOUN'))], [('in', 'ADP'), (('2015', 'ADJ'), ('2015', 'NUM'))], [('2015', 'NUM'), (('that', 'ADP'), ('that', 'DET'))], [('gave', 'VERB'), (('Google', 'ADJ'), ('Google', 'NOUN'))], [('to', 'PRT'), (('Twitter', 'ADJ'), ('Twitter', 'NOUN'))], [(\"'s\", 'PRT'), (('firehose', 'ADJ'), ('firehose', 'NOUN'))], [('.', '.'), (('Twitter', 'ADJ'), ('Twitter', 'NOUN'))], [('users', 'NOUN'), (('post', 'NOUN'), ('post', 'VERB'))], [('and', 'CONJ'), (('interact', 'ADJ'), ('interact', 'VERB'))], [('with', 'ADP'), (('messages', 'ADJ'), ('messages', 'NOUN'))], [('messages', 'NOUN'), (('known', 'ADJ'), ('known', 'VERB'))], [('as', 'ADP'), (('tweets', 'ADJ'), ('tweets', 'NOUN'))], [('television', 'NOUN'), (('personality', 'ADJ'), ('personality', 'NOUN'))], [('The', 'DET'), (('2018', 'ADJ'), ('2018', 'NUM'))], [('2018', 'NUM'), (('FIFA', 'ADJ'), ('FIFA', 'X'))], [('World', 'NOUN'), (('Cup', 'ADJ'), ('Cup', 'NOUN'))], [('the', 'DET'), (('21st', 'ADJ'), ('21st', 'NUM'))], [('21st', 'NUM'), (('FIFA', 'ADJ'), ('FIFA', 'X'))], [('World', 'NOUN'), (('Cup', 'ADJ'), ('Cup', 'NOUN'))], [('football', 'NOUN'), (('tournament', 'ADJ'), ('tournament', 'NOUN'))], [('tournament', 'NOUN'), (('contested', 'ADJ'), ('contested', 'VERB'))], [('World', 'NOUN'), (('Cup', 'ADJ'), ('Cup', 'NOUN'))], [('round', 'NOUN'), (('trips', 'ADJ'), ('trips', 'NOUN'))], [('would', 'VERB'), (('like', 'ADP'), ('like', 'VERB'))], [('and', 'CONJ'), (('arriving', 'ADJ'), ('arriving', 'VERB'))], [('.', '.'), (('NASA', 'ADJ'), ('NASA', 'X'))], [('NASA', 'X'), (('invited', 'ADJ'), ('invited', 'VERB'))], [('to', 'PRT'), (('experience', 'NOUN'), ('experience', 'VERB'))], [('of', 'ADP'), (('ICESAT-2', 'ADJ'), ('ICESAT-2', 'X'))], [('ICESAT-2', 'X'), (('Satellite', 'ADJ'), ('Satellite', 'NOUN'))]]\n"
     ]
    }
   ],
   "source": [
    "#Get the word and tag from the predicted result\n",
    "tagged_SimpleVit_pred = [(tup[0],tup[1]) for tup in SimpleVit_pred]\n",
    "\n",
    "# accuracy\n",
    "check_2 = [i for i, j in zip(tagged_SimpleVit_pred, test_pos) if i == j] \n",
    "#print (len(check_2))\n",
    "accuracy_2 = len(check_2)/len(tagged_SimpleVit_pred)\n",
    "print (\"Accuracy for this model is {:.2%}\".format(accuracy_2))\n",
    "\n",
    "#identify the incorrect words\n",
    "incorrect_tagged_cases = [[test_pos[i-1],j] for i, j in \\\n",
    "                          enumerate(zip(tagged_SimpleVit_pred, test_pos)) if j[0]!=j[1]]\n",
    "print ('Number of words which were tagged incorrectly by Simple Vitrebi algorithm - ',len(incorrect_tagged_cases))\n",
    "\n",
    "print (incorrect_tagged_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of Rule tagger + simple Vitrebi on the test set\n",
    "Accuracy for this model is 95.58%\n",
    "Number of words which were tagged incorrectly by Ruletagger + Simple Vitrebi algorithm -  8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this model is 95.58%\n",
      "Number of words which were tagged incorrectly by Ruletagger + Simple Vitrebi algorithm -  8\n",
      "[[('2015', 'NUM'), (('that', 'ADP'), ('that', 'DET'))], [('an', 'DET'), (('online', 'NOUN'), ('online', 'ADJ'))], [('users', 'NOUN'), (('post', 'NOUN'), ('post', 'VERB'))], [('and', 'CONJ'), (('interact', 'NOUN'), ('interact', 'VERB'))], [('a', 'DET'), (('domineering', 'VERB'), ('domineering', 'ADJ'))], [('the', 'DET'), (('21st', 'X'), ('21st', 'NUM'))], [('would', 'VERB'), (('like', 'ADP'), ('like', 'VERB'))], [('to', 'PRT'), (('experience', 'NOUN'), ('experience', 'VERB'))]]\n"
     ]
    }
   ],
   "source": [
    "#Get the word and tag from the predicted result\n",
    "tagged_Rule_Vit_pred = [(tup[0],tup[1]) for tup in Rule_SimpleVit_pred]\n",
    "\n",
    "# accuracy\n",
    "check_2 = [i for i, j in zip(tagged_Rule_Vit_pred, test_pos) if i == j] \n",
    "#print (len(check_2))\n",
    "accuracy_2 = len(check_2)/len(tagged_Rule_Vit_pred)\n",
    "print (\"Accuracy for this model is {:.2%}\".format(accuracy_2))\n",
    "\n",
    "#identify the incorrect words\n",
    "incorrect_tagged_cases = [[test_pos[i-1],j] for i, j in \\\n",
    "                          enumerate(zip(tagged_Rule_Vit_pred, test_pos)) if j[0]!=j[1]]\n",
    "print ('Number of words which were tagged incorrectly by Ruletagger + Simple Vitrebi algorithm - ',len(incorrect_tagged_cases))\n",
    "\n",
    "print (incorrect_tagged_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy of Transition probaility + simple Vitrebi on the test set\n",
    "- Accuracy for this model is 85.08%\n",
    "- Number of words which were tagged incorrectly by Ruletagger + Simple Vitrebi algorithm -  27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this model is 85.08%\n",
      "Number of words which were tagged incorrectly by Transition probability  + Simple Vitrebi algorithm -  27\n",
      "[[('by', 'ADP'), (('Google', 'DET'), ('Google', 'NOUN'))], [('best-selling', 'ADJ'), (('OS', 'NOUN'), ('OS', 'X'))], [('on', 'ADP'), (('smartphones', 'DET'), ('smartphones', 'NOUN'))], [('since', 'ADP'), (('2011', 'DET'), ('2011', 'NUM'))], [('since', 'ADP'), (('2013', 'DET'), ('2013', 'NUM'))], [('in', 'ADP'), (('2015', 'DET'), ('2015', 'NUM'))], [('2015', 'NUM'), (('that', 'ADP'), ('that', 'DET'))], [('gave', 'VERB'), (('Google', 'X'), ('Google', 'NOUN'))], [('to', 'PRT'), (('Twitter', 'VERB'), ('Twitter', 'NOUN'))], [(\"'s\", 'PRT'), (('firehose', 'VERB'), ('firehose', 'NOUN'))], [('an', 'DET'), (('online', 'NOUN'), ('online', 'ADJ'))], [('users', 'NOUN'), (('post', 'NOUN'), ('post', 'VERB'))], [('and', 'CONJ'), (('interact', 'NOUN'), ('interact', 'VERB'))], [('with', 'ADP'), (('messages', 'DET'), ('messages', 'NOUN'))], [('as', 'ADP'), (('tweets', 'DET'), ('tweets', 'NOUN'))], [('a', 'DET'), (('domineering', 'NOUN'), ('domineering', 'ADJ'))], [('The', 'DET'), (('2018', 'NOUN'), ('2018', 'NUM'))], [('2018', 'NUM'), (('FIFA', 'NOUN'), ('FIFA', 'X'))], [('the', 'DET'), (('21st', 'NOUN'), ('21st', 'NUM'))], [('21st', 'NUM'), (('FIFA', 'NOUN'), ('FIFA', 'X'))], [('tournament', 'NOUN'), (('contested', 'NOUN'), ('contested', 'VERB'))], [('would', 'VERB'), (('like', 'ADP'), ('like', 'VERB'))], [('and', 'CONJ'), (('arriving', 'NOUN'), ('arriving', 'VERB'))], [('.', '.'), (('NASA', 'NOUN'), ('NASA', 'X'))], [('NASA', 'X'), (('invited', 'NOUN'), ('invited', 'VERB'))], [('to', 'PRT'), (('experience', 'NOUN'), ('experience', 'VERB'))], [('of', 'ADP'), (('ICESAT-2', 'DET'), ('ICESAT-2', 'X'))]]\n"
     ]
    }
   ],
   "source": [
    "#Get the word and tag from the predicted result\n",
    "tagged_TransP_Vit_pred = [(tup[0],tup[1]) for tup in TransP_SimpleVit_pred]\n",
    "\n",
    "# accuracy\n",
    "check_2 = [i for i, j in zip(tagged_TransP_Vit_pred, test_pos) if i == j] \n",
    "#print (len(check_2))\n",
    "accuracy_2 = len(check_2)/len(tagged_TransP_Vit_pred)\n",
    "print (\"Accuracy for this model is {:.2%}\".format(accuracy_2))\n",
    "\n",
    "#identify the incorrect words\n",
    "incorrect_tagged_cases = [[test_pos[i-1],j] for i, j in \\\n",
    "                          enumerate(zip(tagged_TransP_Vit_pred, test_pos)) if j[0]!=j[1]]\n",
    "print ('Number of words which were tagged incorrectly by Transition probability  + Simple Vitrebi algorithm - ',len(incorrect_tagged_cases))\n",
    "\n",
    "print (incorrect_tagged_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of the Incorrect words across different algorithms\n",
    "#### Simple Viterbi Model\n",
    "- Accuracy of ``77.9 %`` on the test set. This accuracy will depend on the tag which is first in the unique tags list.In case this turns out to be a Noun, the accuracy shoots up to almost ``85%``.\n",
    "- ``Words such as 'Android','Google' 'OS','worldwide','smartphones'  '2011', '2013','Twitter', '2015',\"'s\",'firehose' , 'online', 'messages','tweets','post', 'interact', 'domineering','personality', '2018', 'FIFA','Cup',  '21st','tournament', 'trips', 'contested', 'like',  'arriving', 'NASA', 'invited', 'ICESAT-2', 'experience','Satellite' are not tagged correctly``\n",
    "\n",
    "#### Viterbi+Ruletagger\n",
    "In the combination of Viterbi+Ruletagger words such as the below are correctly tagged - Here due to rules defined most of the words get tagged accuratly. The last rule defined is to mark all unknown words as Noun\n",
    "\n",
    "- ``Accuracy of ~96% on the test data``\n",
    "- ``'Android','Google', 'worldwide', 'smartphones', 'Twitter', 'firehose', 'messages', 'tweets', 'personality', 'tournament', 'trips' are correctly identified as Noun``\n",
    "- `` 2011, 2013, 2015 ,2018  are identified as NUM. `` Numbers which could not be classified in the simple viterbi algorithm are classified correctly\n",
    "- `` OS ,21st ,'ICESAT-2 ,FIFA identifed as X. `` Words are corrctly tagged as 'X' which was not the case in the Simple Viterbi algorithm.\n",
    "- `` contested ,  arriving ,invited are correctly identified as VERB. `` These wors were not correctlyidentified in the simple Vitrebi algorithm\n",
    "- Words such as ``that ,online ,post, interact, domineering, 21st, like and experience`` were not identified correctly\n",
    "\n",
    "\n",
    "#### Viterbi+TransitionProb Algorithm\n",
    "In the combination of Viterbi+TransitionProb the accuracy is less than the simple Viterbi , since there is no mechanism of setting a default tag for the word. However this will only be true if the NOUNs forma majority. In case this is not the case the accuracy for this algorithm will be higher.\n",
    "\n",
    "- ``Accuracy of ~85% on the test data``\n",
    "``words such as the below are correctly tagged -``\n",
    "- `` Andriod, Twitter (1 instance),worldwide ,personality ,Cup,tournament,Satellite are identified as Noun`` These were incorreclty identified by the simple Viterbi algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns = [\n",
    "#     (r'.*ing$', 'VERB'),          #words ending with ing    \n",
    "#     (r'.*ed$', 'VERB'),           #words ending with ed\n",
    "#     (r'\\d+(\\.\\d*)?$', 'NUM'),\n",
    "#     #(r'[0-9]+$', 'NUM'),     #words which are number\n",
    "#     (r'.*[0-9]+.*', 'X'),         #words which have text, symbol  and numbers are classified as X\n",
    "#     (r'[A-Z]{1,4}$','X'),         #words which are abbrevition upto 4 chars\n",
    "#     (r'.*', 'NOUN')               #All others are classified as noun     \n",
    "# ]\n",
    "# word = [\"0.\"]\n",
    "# regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "# print( regexp_tagger.tag(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
